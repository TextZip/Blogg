<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://textzip.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://textzip.github.io/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2024-12-14T15:49:41+05:30</updated><id>https://textzip.github.io/feed.xml</id><title type="html">Jai Krishna</title><subtitle>A portfolio of my adventures in robotics, electronics and mechanical along with tutorials on topics related to robotics.</subtitle><author><name>Jai Krishna</name></author><entry><title type="html">Proprioceptive Locomotion in Unstructured Environments</title><link href="https://textzip.github.io/posts/Loco-DRL/" rel="alternate" type="text/html" title="Proprioceptive Locomotion in Unstructured Environments" /><published>2023-05-08T13:13:20+05:30</published><updated>2023-05-08T13:13:20+05:30</updated><id>https://textzip.github.io/posts/Loco-DRL</id><content type="html" xml:base="https://textzip.github.io/posts/Loco-DRL/"><![CDATA[<p><img src="/assets/img/Loco-DRL/cover.png" alt="Image1" class="shadow" /></p>

<p>The following work has been done during my time at the <a href="https://unit.aist.go.jp/jrl-22022/index_en.html">CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan</a> for my undergraduate thesis under the supervision of <a href="https://unit.aist.go.jp/jrl-22022/en/members/member-morisawa.html">Dr. Mitsuharu Morisawa</a> with support from <a href="https://unit.aist.go.jp/jrl-22022/en/members/member-singh.html">Rohan Singh</a>.</p>

<!-- <iframe width="640" height="385" src="https://youtube.com/embed/Mq8utqI5-_g" frameborder="0" allowfullscreen></iframe> -->

<blockquote>
  <p><strong>Note:</strong> Detailed results and video clips can be found in the <a href="#results">Results</a> section below.</p>
</blockquote>

<p>To be updated soon</p>]]></content><author><name>Jai Krishna</name></author><category term="Projects" /><category term="Quadrupeds" /><category term="reinforcement learning" /><category term="sim2real" /><category term="quadruped" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/Loco-DRL/cover2.png" /><media:content medium="image" url="https://textzip.github.io/assets/img/Loco-DRL/cover2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods</title><link href="https://textzip.github.io/posts/DRL-4/" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods" /><published>2023-01-23T13:13:20+05:30</published><updated>2023-01-24T14:55:44+05:30</updated><id>https://textzip.github.io/posts/DRL-4</id><content type="html" xml:base="https://textzip.github.io/posts/DRL-4/"><![CDATA[<p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>

<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
  <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em></li>
  <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em></li>
  <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</li>
  <li>Part 7 - A2C, PPO, TRPO, GAE, A3C</li>
  <li>TBA (HER, PER, Distillation)</li>
</ul>

<h1 id="monte-carlo-methods">Monte Carlo Methods</h1>
<p>MC methods improvise over DP methods as they can be used in cases where we do not have a model of the environment. They do this by learning from episodes of experience. Therefore one caviat of MC methods is that they do not work on continous MDPs and learn only from complete episodes (Episodes must terminate).</p>

<h2 id="monte-carlo-prediction">Monte Carlo Prediction</h2>
<p>We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.</p>
<h3 id="on-policy-mc-prediction">On-Policy MC Prediction</h3>
<p>We have a small variation in the fact that we can consider each state only the first time it has been visited while estimating the mean return or we can account for multiple visits to the same state(if any) and the follwoing pseudocodes illustrate both the variations 
<img src="/assets/img/DRL4/first-visit-mc-pred.png" alt="image1" class="shadow" /></p>

<p>The every-visit version can be implemented by removing the “Unless $S_t$ appears in $S_0$, $S_1$, … $S_{t-1}$” line.</p>

<h4 id="incremental-updates">Incremental Updates</h4>
<p>A more computationally efficient method would be to calculate the mean incrementally as follows:</p>

<ul>
  <li>Update $V(s)$ incrementally after episode $S_1,A_1,R_2,….,S_T$</li>
  <li>For each state $S_t$ with return $G_t$</li>
</ul>

\[N(S_t) \leftarrow  N(S_t) + 1\]

\[V(S_t) \leftarrow  V(S_t) + \dfrac{1}{N(S_t)}(G_t-V(S_t))\]

<p>For non-stationary problems, it can be useful to track a running mean (forgets old epiosdes and gives more weight to recent experiences).</p>

\[V(S_t) \leftarrow  V(S_t) + \alpha(G_t-V(S_t))\]

<h3 id="off-policy-mc-prediction">Off-Policy MC Prediction</h3>
<p>Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Off-Policy Monte Carlo Prediction can be implemented via two variations of importance sampling which are discussed below.</p>
<h4 id="ordinary-importance-sampling">Ordinary Importance Sampling</h4>
<p>For evaluating a terget policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s,a)$ while following a behaviour policy $\mu(a|s)$.</p>

<p>Given, 
\(\{S_1,A_1,R_2,...,S_T\} \sim \mu\)</p>

<p>We can weight returns $G_t$ according to similarity between the two policies. By multiplying the importance sampling corrections along the whole episode we get:</p>

\[G_t^{\pi/\mu} = \dfrac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})...\pi(A_{T}|S_{T})}{\mu(A_{t}|S_{t})\mu(A_{t+1}|S_{t+1})...\mu(A_{T}|S_{T})}G_t\]

<p>We can then update the state value towards the corrected return like this</p>

\[V(S_t) \leftarrow  V(S_t) + \alpha(G_t^{\pi/\mu}-V(S_t))\]

<p>Note that we cannot use this if $\mu$ is zero when $\pi$ is non-zero, also that importance sampling can increase variance.</p>

<h4 id="weighted-importance-sampling">Weighted Importance Sampling</h4>
<p><img src="/assets/img/DRL4/off-policy-mc-weight-sample-pred.png" alt="image1" class="shadow" />
The derivation of the Weighted Importance Sampling equations has been left-out for the time being.</p>

<p>Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges
asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and
Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.</p>
<h2 id="monte-carlo-control">Monte Carlo Control</h2>
<p>While it might seem straight forward to implement MC Methods in GPI by plugging MC Prediction for policy evaluation and using greedy policy improvement to complete the cycle, there is one key problem that needs to be addressed.</p>

<p>Greedy policy improvement over $V(s)$ requires knowledge about the MDP
\(\pi'(s) = \mathtt{argmax}_{a\in A}  r(a|s) + p(s'|s,a)V(s')\)</p>

<p>To remain model free we can instead switch to action value functions which will not require prior details about the MDP</p>

\[\pi'(s) = \mathtt{argmax}_{a\in A} Q(s,a)\]

<p>While this solves the issue of knowing the model MDP, we now have a deterministic policy and we will never be able to collect experiences of alternative actions and therefore might miss out on exploration altogether.</p>

<p>This can be solved in the following ways:</p>

<ul>
  <li>
    <p><strong>Exploring Starts:</strong> Every state-action pair has a non-zero probability of being selected as the starting pair, this ensures sufficient exploration but in reality, this might not always be possible.</p>
  </li>
  <li>
    <p><strong>$\epsilon-$ soft policies:</strong> A small probability to explore every time an action is to be choosen.</p>
  </li>
  <li>
    <p><strong>Off-Policy:</strong> Use a different policy to collect experience than the one target policy being improved.</p>
  </li>
</ul>

<h3 id="on-policy-mc-control">On-Policy MC Control</h3>
<h4 id="exploring-starts">Exploring Starts</h4>
<p>The pseudocode for exploring starts can be found below:
<img src="/assets/img/DRL4/mc-es-control.png" alt="image1" class="shadow" /></p>

<h4 id="on-policy-first-visit-mc-control">On-Policy First Visit MC Control</h4>
<p>The pseudocode for On-Policy First Visit MC Control can be found below:
<img src="/assets/img/DRL4/on-policy-fv-mc-control.png" alt="image1" class="shadow" /></p>

<p>The python implementation for the following can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">sa_returns</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sa_returns</span><span class="p">:</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)].</span><span class="nf">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h4 id="on-policy-every-visit-mc-control">On-Policy Every Visit MC Control</h4>
<p>On-Policy Every Visit MC Control can be implemented by making a small change to the inner loop of the above code for the first visit version as follows:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">sa_returns</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sa_returns</span><span class="p">:</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)].</span><span class="nf">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="on-policy-every-visit-constant-alpha-mc-control">On-Policy Every Visit Constant Alpha MC Control</h4>
<p>The constant alpha version is based on the idea of using a running mean instead of using a normal return to deal with non-stationary problems.</p>

<p>The major change being the following equation: 
\(Q(S_t|A_t) \leftarrow  Q(S_t|A_t) + \alpha(G_t-Q(S_t|A_t))\)</p>

<p>The python implementation can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">constant_alpha_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="n">old_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">G</span><span class="o">-</span><span class="n">old_value</span><span class="p">)</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">constant_alpha_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="off-policy-mc-control">Off-Policy MC Control</h3>
<p>In Off-Policy methods, the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.</p>

<p>Off-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).</p>

<p><img src="/assets/img/DRL4/off-policy-mc-control.png" alt="image1" class="shadow" /></p>

<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">exploratory_policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">off_policy_monte_carlo</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span><span class="n">target_policy</span><span class="p">,</span><span class="n">exploratory_ploicy</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">counter_sa_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span> 

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">exploratory_ploicy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span>     

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>
            <span class="n">counter_sa_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span>
            <span class="n">old_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">W</span><span class="o">/</span><span class="n">counter_sa_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">])</span><span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">old_value</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">action_t</span> <span class="o">!=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state_t</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="nf">off_policy_monte_carlo</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">target_policy</span><span class="o">=</span><span class="n">target_policy</span><span class="p">,</span><span class="n">exploratory_ploicy</span><span class="o">=</span><span class="n">exploratory_policy</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="temporal-difference-methods">Temporal Difference Methods</h1>
<p>Temporal Difference methods improvise over MC methods by learning from incomplete episodes of experience using bootstrapping.</p>
<h2 id="temporal-difference-prediction">Temporal Difference Prediction</h2>
<p>The simplest temporal-difference learning algorithm TD(0) works as follows:</p>

<p>The $V(S_t)$ can be updated using the actual return $G_t$</p>

\[V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))\]

<p>We can replace the actual return $G_t$ with the estimated return $R_{t+1} + \gamma V(S_{t+1})$ as follows</p>

\[V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))\]

<p>Where,</p>

<p>$R_{t+1} + \gamma V(S_{t+1})$ is called the TD target</p>

<p>$\delta_t =  R_{t+1} + \gamma V(S_{t+1}) - V(S_t) $ is called the TD error</p>

<h3 id="on-policy-td-prediction">On-Policy TD Prediction</h3>
<p>On-Policy TD prediction can be implemented as explained below
<img src="/assets/img/DRL4/TD-0-pred-online.png" alt="image1" class="shadow" /></p>

<h3 id="off-policy-td-prediction">Off-Policy TD Prediction</h3>
<p>Using Importance Sampling for Off-Policy Learning, we can implement TD Prediction as follows:</p>

<p>Use TD targets generated from $\mu$ to evaluate $\pi$. We can weight the TD target $(R + \gamma V(S’))$ by the importance sampling ratio. Note that we only need to correct a single instane of the prediction unlike MC methods where the whole episde has to be corrected.</p>

\[V(S_t) \leftarrow V(S_t) + \alpha \left( \dfrac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1} + \gamma V(S_{t+1})) - V(S_t) \right)\]

<p>Note that the variance is much lower than MC importance sampling</p>

<h2 id="temporal-difference-control">Temporal Difference Control</h2>
<p>The basic stratergy for using TD methods for control is to plug them into the GPI framework for policy evaluation by learning action values to remain model-free.</p>

<p>The general update rule for action value estimation can be written as:</p>

\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]\]

<h3 id="on-policy-td-control-sarsa">On-Policy TD Control (SARSA)</h3>
<p>The On-Policy TD Control method is also referred to as SARSA as it uses the quintuple of events $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ for every update.</p>

<p>The pseudocode for SARSA can be found below
<img src="/assets/img/DRL4/sarsa.png" alt="image1" class="shadow" /></p>

<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">sarsa</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>  <span class="c1"># on-policy-td-learning
</span>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">termination</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">termination</span> <span class="ow">or</span> <span class="n">truncation</span>
            <span class="n">next_action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">next_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

            <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
            <span class="n">next_action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_value</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">next_action_value</span> <span class="o">-</span> <span class="n">action_value</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">sarsa</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="off-policy-td-control-q-learning">Off-Policy TD Control (Q-Learning)</h3>
<p>Off-Policy TD Control is often referred to as Q-Learning and does not require importance sampling. For TD(0) based Q-Learning, since the action $A_t$ is already determined the  importance sampling ratio essentially becomes 1 and therefore can be ignored.</p>

<p>The next action is chosen using a behaviour policy $A_{t+1} \sim \mu(.|S_t)$ which is $\epsilon$-greedy w.r.t $Q(S,A)$. But we consider alternative successor action $a \sim \pi(.|S_t)$ using the target policy $\pi$ which is greedy w.r.t $Q(S,A)$</p>

<p>The update rule can be written down as follows:</p>

\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \mathtt{max}_{a} Q(S_{t+1},a) - Q(S_t,A_t)]\]

<p>The pseudocode for Q-Learning can be found below:
 <img src="/assets/img/DRL4/q-learning.png" alt="image1" class="shadow" /></p>

<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">exploratory_policy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">q_learning</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">target_policy</span><span class="p">,</span> <span class="n">exploratory_policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>  <span class="c1"># on-policy-td-learning
</span>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">exploratory_policy</span><span class="p">()</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">termination</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">termination</span> <span class="ow">or</span> <span class="n">truncation</span>
            <span class="n">next_action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">next_state</span><span class="p">)</span>

            <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
            <span class="n">next_action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_value</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">next_action_value</span> <span class="o">-</span> <span class="n">action_value</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="nf">q_learning</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_policy</span><span class="o">=</span><span class="n">target_policy</span><span class="p">,</span> <span class="n">exploratory_policy</span><span class="o">=</span><span class="n">exploratory_policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="expected-sarsa">Expected SARSA</h3>
<p>Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule as follows:</p>

\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \mathbf{E}_\pi[Q(S_{t+1},A_{t+1} | S_{t+1})] - Q(S_t,A_t) ]\]

<p>which can be written as</p>

\[Q(S_t,A_t) = Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \Sigma_a\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t,A_t) ]\]

<p>Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of At+1. Given the same amount of experience we might expect it to perform slightly better than Sarss.</p>

<h1 id="bootstrapping">Bootstrapping</h1>
<p>While TD(0) methods take one step and estimate the return, MC methods wait till the end of the episode to calculate the return. While both these methods might appear to be very different they can unified by using TD methods to look n-steps into the future, as shown by the image below.
<img src="/assets/img/DRL4/n-step-td.png" alt="image1" class="shadow" /></p>

<!-- ## n-step TD

## n-step SARSA -->

<!-- ## TD$(\lambda)$


## SARSA$(\lambda)$ -->]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Deep Reinforcement Learning" /><category term="mdp" /><category term="optimal value" /><category term="bellman" /><summary type="html"><![CDATA[This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/drl_logo.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/drl_logo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning - Part 3 - Dynamic Programming</title><link href="https://textzip.github.io/posts/DRL-3/" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Part 3 - Dynamic Programming" /><published>2023-01-20T13:13:20+05:30</published><updated>2023-01-23T16:42:20+05:30</updated><id>https://textzip.github.io/posts/DRL-3</id><content type="html" xml:base="https://textzip.github.io/posts/DRL-3/"><![CDATA[<p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>

<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
  <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em></li>
  <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em></li>
  <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</li>
  <li>Part 7 - A2C, PPO, TRPO, GAE, A3C</li>
  <li>TBA (HER, PER, Distillation)</li>
</ul>

<h1 id="dynamic-programming">Dynamic Programming</h1>
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the Approx. methods presented in later parts. In
fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p>

<h2 id="policy-evaluation">Policy Evaluation</h2>
<p>First we consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.</p>

<p>Recall that,</p>

\[v_\pi(s) = \sum_{a}\pi(a\mid s)\sum_{s',r} p(s',r\mid a,s)[r+\gamma v_\pi(s')]\]

<p>We can evaluate a given policy $\pi$ by iterativly applying the bellman expectation backup as an update rule until the value function $v(s)$ converges to the $v_\pi(s)$.</p>

<h2 id="policy-improvement">Policy Improvement</h2>
<p>We can then improve a given policy by acting greedily with respect to the given value function for the policy $v_\pi(s)$.</p>

<p>The new policy $\pi’$ is better than or equal to the old policy $\pi$. Therefore, $\pi’ \ge \pi$.</p>

<p>If the improvement stops</p>

\[q_\pi(s,\pi'(s)) = \mathtt{max}_{a \in A} \  q_\pi(s,a) \ =   q_\pi(s,\pi(s)) = v_\pi(s)\]

<p>Therefore,</p>

\[v_\pi(s) =  \mathtt{max}_{a \in A}  q_\pi(s,a)\]

<p>which is the bellman optimality equation and $v_\pi(s) = v_\star(s)$.</p>

<h2 id="policy-iteration">Policy Iteration</h2>
<p>Policy Iteration combines the evaluation and improvement steps into a single algorithm where a random policy is taken, its evaluated and then improved upon and the resulting policy is again evaluated and then improved upon and so on until the policy finally converges and becomes the optimal policy.</p>

<p>Policy Iteration is also refered to as the control problem in DP litrature as opposed to the prediction problem that is policy evaluation.</p>

<p><img src="/assets/img/DRL3/Policy_Iteration.png" alt="image1" class="shadow" /></p>

<p>The algorithm can be summaried as follows:</p>

<p><img src="/assets/img/DRL3/PI_algo.png" alt="image1" class="shadow" /></p>

<p>The $\Delta$ in the above code is used to determine the accuracy of the estimation and can be a value close to 0.</p>

<p>The python code for Policy Iteration is as follows:</p>

<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># state-action table
</span><span class="n">policy_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">),</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">state_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="n">policy_stable</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="c1"># policy eval
</span>        <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">old_value</span> <span class="o">=</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
                <span class="n">new_state_value</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                    <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">new_state_value</span> <span class="o">+=</span> <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span><span class="o">*</span><span class="p">(</span>
                        <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_state_value</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_value</span><span class="o">-</span><span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>
        <span class="c1"># policy improvement
</span>        <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
            <span class="n">old_action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
            <span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
            <span class="n">max_q</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">probablity</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_q</span><span class="p">:</span>
                    <span class="n">max_q</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
                    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
                    <span class="n">action_probs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>
        <span class="c1"># check termination condition and update policy_stable variable
</span>            <span class="k">if</span> <span class="n">old_action</span> <span class="o">!=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">):</span>
                <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">False</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>


<span class="nf">policy_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="o">=</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="o">=</span><span class="n">state_values</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Done</span><span class="sh">"</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Another important point is the fact that we do not need to wait for the policy evaluation to converge to $v_\pi$ before performing policy improvement. Therefore, a stopping condition can be introduced without effecting the performance.</p>

<p>When the policy evaluation step is stopped after a single step, k = 1 we arrive at a special case of policy evaluation which is equivalent to another method called value iteration.</p>
<h2 id="value-iteration">Value Iteration</h2>
<p>The bellman optimality backup equation can be applied iteratively until convergence to arrive at the optimal value function $v_\star(s)$. Unlike policy iteration the itermediate value functions do not correspond to any policy. 
The algorithm can be summaried as follows:</p>

<p><img src="/assets/img/DRL3/VI_algo.png" alt="image1" class="shadow" />
The python code for Value Iteration is as follows:</p>

<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>

<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># state-action table
</span><span class="n">policy_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">),</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">state_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
            <span class="n">old_value</span> <span class="o">=</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
            <span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
            <span class="n">max_q</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">probablity</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_q</span><span class="p">:</span>
                    <span class="n">max_q</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
                    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
                    <span class="n">action_probs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_q</span>
            <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>

            <span class="n">delta</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_value</span> <span class="o">-</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>


<span class="nf">value_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="o">=</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="o">=</span><span class="n">state_values</span><span class="p">)</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Bellman Equation</th>
      <th>Algorithm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prediction</td>
      <td>Bellman Expectation Equation</td>
      <td>Iterative Policy Evaluation</td>
    </tr>
    <tr>
      <td>Control</td>
      <td>Bellman Expectation Equation + Greedy Policy Improvement</td>
      <td>Policy Iteration</td>
    </tr>
    <tr>
      <td>Control</td>
      <td>Bellman Optimality Equation</td>
      <td>Value Iteration</td>
    </tr>
  </tbody>
</table>

<h2 id="async-dynamic-programming">Async. Dynamic Programming</h2>
<p>All the methods discussed till now were synchronous in nature and at each step of the iteration we used a loop to go over all the states. We can however also asynchronously backup states in any order and this will also lead to the same solution as long as all the states are selected atleast once. The added advantage is that this greatly reduces computational time and gives rise to methods like prioritised sweeping and others.</p>

<h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2>
<p>We use the term generalized policy iteration (GPI) to refer
to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.</p>

<h2 id="pros-and-cons-of-dynamic-programming">Pros and Cons of Dynamic Programming</h2>

<p>DP is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact that the number of states often grows exponentially with the number
of state variables. Large state sets do create diculties, but these are inherent difficulties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming.</p>

<p>But DP methods assume we know the dynamics of the environment which is one of the biggest limiting factors for their direct use in many cases. In the upcoming parts we will loot at methods that tackle this issue.</p>]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Deep Reinforcement Learning" /><category term="mdp" /><category term="optimal value" /><category term="bellman" /><summary type="html"><![CDATA[This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/drl_logo.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/drl_logo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning - Part 0 - Getting Started</title><link href="https://textzip.github.io/posts/DRL-0/" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Part 0 - Getting Started" /><published>2023-01-18T13:13:20+05:30</published><updated>2023-01-23T16:42:20+05:30</updated><id>https://textzip.github.io/posts/DRL-0</id><content type="html" xml:base="https://textzip.github.io/posts/DRL-0/"><![CDATA[<p>Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Learning for Robotics can be broken down into three almost seperate parts:</p>
<ul>
  <li>Deep Learning (Neural Networks)</li>
  <li>Reinforcement Learning Algorithm</li>
  <li>Choice of State Space, Action Space and Reward Functions</li>
</ul>

<h1 id="how-to-get-started">How To Get Started</h1>
<p>There are a finite number of ways to get started with DRL, but I believe that the right curriculum (learning xD) can make you go a long way.</p>

<p>RL Agents pretty much wither and die when left in environments with sparse rewards. One of the easiest solution for both RL Agents and beginners trying to learn DRL is to have a dense reward function that introduces a lot of fun and rewards at every timestep in the journey.</p>

<h3 id="target-audiance">Target-Audiance</h3>
<p>People who have prior experience with python, numpy(preferably) and basic understanding of probability and statistics. Added benefit if you are comfortable with basic calculus.</p>

<p>You can learn the maths and numpy on the go as and when it is required but a decent understanding of python datatypes, object-oriented-programming is expected.</p>

<h3 id="end-goal">End-Goal</h3>
<p>The end-goal for this curriculum is to equip you with the skills required to read through research papers and reimplement/modify them, understand opensource projects and ultimately help you get started with research in DRL.</p>

<p>If this is not what your looking for then this probably isn’t the right curriculum for you.</p>

<h2 id="curriculum">Curriculum</h2>
<p>The curriculum laid out below is my opinon and it might or might not be the best way for you to get into DRL, so please use it accordingly.</p>

<h3 id="system-setup">System Setup</h3>
<p>Please create a virtualenv and switch to python 3.8 for the entire series. Linux(Ubuntu or any other distro) is the recommended OS, while some of the code and packages might work in windows, I will not be helping with any windows debugging.</p>

<h2 id="phase-one">Phase One</h2>
<h3 id="1-get-started-with-gymnasium"><strong>1. Get started with gymnasium</strong></h3>

<p><strong>Prerequisites</strong>: <em>python</em></p>

<p><strong>Note</strong>: If you are an existing user of gym please refer to the migration guide to gymnasium <a href="https://gymnasium.farama.org/content/migration-guide/">here</a>.</p>

<ul>
  <li>
    <p>Explore the structure of <a href="https://gymnasium.farama.org/content/basic_usage/">gymnasium-api</a>. Render a couple of environments until your comfortable with the syntax and have a general idea of what is happening in the code.</p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-1 | Solution-1</p>
  </li>
</ul>

<h3 id="2-play-around-with-rewards-states-and-actions"><strong>2. Play around with rewards, states and actions</strong></h3>

<p><strong>Prerequisites</strong>: <em>python, gymnasium</em></p>

<ul>
  <li>
    <p>Get started with a 3rd party RL Library (<a href="https://stable-baselines3.readthedocs.io/en/master/index.html">SB3</a>, <a href="https://docs.cleanrl.dev/get-started/basic-usage/#get-documentation">CleanRL</a>, <a href="https://docs.ray.io/en/latest/rllib/index.html">RLlib</a> or any other implementation of your choice) and a robust RL algorithm like PPO and focus on changing the rewards, states and actions in the <a href="https://gymnasium.farama.org/environments/classic_control/">basic environments</a> to solve them. Gain an intution of how the RL framework works.</p>
  </li>
  <li>
    <p>For robotics in paticular, try the <a href="https://gymnasium.farama.org/environments/mujoco/">MuJoCo environments</a> like Ant or check <a href="https://github.com/clvrai/awesome-rl-envs">here</a> and <a href="https://github.com/kengz/awesome-deep-rl">here</a> for other available options (This is not an exhaustive list).</p>
  </li>
  <li>
    <p>What happens when you use torque instead of position in the action space ? What happens when you given a combination of negative and postive rewards ? What are termination conditions ?</p>
  </li>
  <li>
    <p>Believe it or not, you are already in a position to replicate a couple of basic DRL papers. Search for papers that are related to blind locomotion in quadrupeds or robotic manipulators (for example), you should be able to comfortably work with any paper that involve changes to only the state, action and rewards.</p>
  </li>
  <li>
    <p>Try importing different robot models into MuJoCo or any other physics engine of your choice and getting them to work or alternativly use one of the above listed rl-envs. Here here a couple of papers that you can implement along with a link to my implementation, feel free to try it on your own first or tinker around with my code directly:</p>

    <blockquote>
      <p>Fu, Z., Kumar, A., Malik, J., &amp; Pathak, D. (2021). <a href="https://arxiv.org/pdf/2111.01674.pdf">Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots.</a> doi:10.48550/ARXIV.2111.01674</p>
    </blockquote>

    <blockquote>
      <p>Franceschetti, A., Tosello, E., Castaman, N., &amp; Ghidoni, S. (2020). <a href="https://arxiv.org/abs/2005.02632">Robotic Arm Control and Task Training through Deep Reinforcement Learning.</a> doi:10.48550/ARXIV.2005.02632</p>
    </blockquote>

    <blockquote>
      <p>Michel Aractingi, Pierre-Alexandre Léziart, Thomas Flayols, Julien Perez, Tomi Silander, et al.. <a href="https://hal.laas.fr/hal-03761331/document">Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning.</a> 2022. ⟨hal-03761331⟩</p>
    </blockquote>

    <blockquote>
      <p>Fang-I Hsiao, Cheng-Min Chiang, Alvin Hou, et al.. <a href="https://web.stanford.edu/class/aa228/reports/2019/final62.pdf">Reinforcement Learning Based Quadcopter Controller</a></p>
    </blockquote>
  </li>
  <li>
    <p>You can even try ideas like curriculum learning, dynamic goal generation and other ideas that vary the difficult of the training as per the agents performance.</p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-2 | Solution-2</p>
  </li>
</ul>

<h3 id="3-learn-tabular-reinforcement-learning-methods"><strong>3. Learn Tabular Reinforcement Learning Methods</strong></h3>

<p><strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></p>

<p><strong>NOTE</strong>: <em>You can continue to explore ideas and research papers from step 2 in parallel.</em></p>

<ul>
  <li>
    <p>Learn about the basics/fundamentals of reinforcement learning mainly: K-Arm Bandits, MDP, Monte-Carlo Methods, Temporal Difference Methods, Bootstrapping</p>
  </li>
  <li>
    <p>Refer to the <a href="#sources--references">Sources &amp; References</a> for links to external resources like video lectures.</p>
  </li>
  <li>
    <p>Refer to the following sections of the blog series for code and theory:</p>

    <blockquote>
      <p><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></p>
    </blockquote>

    <blockquote>
      <p><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a></p>
    </blockquote>

    <blockquote>
      <p><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a></p>
    </blockquote>

    <blockquote>
      <p>Part 4 - Monte Carlo and Temporal Difference Methods</p>
    </blockquote>
  </li>
  <li>
    <p>Solve some of the basic low dimenssional problems from the gym environments like <a href="https://gymnasium.farama.org/environments/toy_text/">toy-text</a> problems</p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-3 | Solution-3</p>
  </li>
</ul>

<h2 id="phase-two">Phase Two</h2>

<h3 id="4-deep-learning-framework"><strong>4. Deep Learning Framework</strong></h3>
<p><strong>Prerequisites</strong>: <em>python, numpy</em></p>

<ul>
  <li>
    <p>Pick a library of your choice for learning Neural Networks, this guide will be based on PyTorch. (Other options like TensorFlow exist, pick whatever works best for you.)</p>
  </li>
  <li>
    <p>Learn Deep Learning using PyTorch. In paticular try a couple of basic projects till your comfortable with the following ideas: Loss Functions, Activation Functions, PyTorch Syntax, MLP, CNN, RNN, LSTM, GAN, Autoencoders, Weight Initializations, Dropout, Optimizers.</p>
  </li>
  <li>
    <p>Refer to the <a href="#sources--references">Sources &amp; References</a> for links to external resources for learning.</p>
  </li>
  <li>
    <p>Do a couple of pure Deep-Learning projects like binary/multi-class classification, De-noising Images and so on..</p>
  </li>
  <li>
    <p>Try going through some of the classic papers in DL that laid the foundation for modern DL. <a href="https://github.com/TextZip/drl-resources">Here</a> is a link to my collection of must read classics. Here are links to an external collection that is more exhaustive <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html#bonus-classic-papers-in-rl-theory-or-review">spinning-openai</a>, <a href="https://github.com/tigerneil/awesome-deep-rl">awesome-deep-rl</a>, <a href="https://github.com/jgvictores/awesome-deep-reinforcement-learning">awesome-deep-reinforcement-learning</a></p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-4 | Solution-4</p>
  </li>
</ul>

<h3 id="5-apply-deep-learning-to-rl"><strong>5. Apply Deep Learning to RL</strong></h3>
<p><strong>Prerequisites</strong>: <em>python, numpy, rl-library-of-your-choice</em></p>

<ul>
  <li>
    <p>Make small changes to the exisiting neural networks from your prior projects in Section <a href="#2-play-around-with-rewards-states-and-actions">2. Play around with rewards, states and actions</a> which were based on 3rd party RL libraries.</p>
  </li>
  <li>
    <p>Change the number of layers, the type of NNet, the activation functon, maybe add a CNN and take camera input in the state.</p>
  </li>
  <li>
    <p>Upgrade projects you worked on earlier like quadruped/robotics arm by adding camera inputs to the state space or change the NNet type to RNNs or LSTMs and check how the performace of the agent changes.</p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-5 | Solution-5</p>
  </li>
</ul>

<h3 id="6-approx-methods-in-rl"><strong>6. Approx. Methods in RL</strong></h3>
<p><strong>Prerequisites</strong>: <em>python, numpy, PyTorch</em></p>

<ul>
  <li>
    <p>Learn Deep Q-Learning, Policy Gradient, Actor-Critic Methods and other algorithms and implement them.</p>
  </li>
  <li>
    <p>Refer to the following sections of the blog series:</p>

    <blockquote>
      <p>Part 5 - Deep SARSA and Q-Learning</p>
    </blockquote>

    <blockquote>
      <p>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</p>
    </blockquote>

    <blockquote>
      <p>Part 7 - A2C, PPO, TRPO, GAE, A3C</p>
    </blockquote>
  </li>
  <li>
    <p>You should now be able to implement a good number of research papers, explore ideas like HER, PER, World Models and other concepts.</p>
  </li>
  <li>
    <p><strong>Progress Check:</strong> Assignmnet-6 | Solution-6</p>
  </li>
</ul>

<h2 id="where-does-this-blog-fit-in-">Where Does This Blog Fit in ?</h2>

<p>Some drawbacks of the existing resources for DRL:</p>

<ul>
  <li>
    <p>Most of them only focus on the theory or the code but not both. A majority of the courses that cover the theory in great detail do not have any coding components making it very difficult to implement any learning. The courses which are coding centric only focus on the code and skip most of the theory and give a vague intution about the proof or the derivation for the formulas used.</p>
  </li>
  <li>
    <p>Many courses use their own custom environments for teaching (ahm ahm Coursera Specialization) while this can make learning/teaching easy. Some use jupyter notebooks for teaching, most if not all the RL libraries and opensource projects in the internet use argparse and write their code in modular file structures. Once you step outside the course sandbox it becomes very difficult to switch or even follow other projects.</p>
  </li>
  <li>
    <p>A good majority of courses are topic specific aka they only teach something with limits scope or prespective in mind. For example, there are tons of Deep Learning courses but there usually isn’t a deep learning for reinforcement learning course. So, you end up learning a lot more than what is needed and the course usually might focus on things that are not really required for DRL.</p>
  </li>
</ul>

<p><strong>The primary goal for this blog series is to bridge the gap between theory and code in Deep Reinforcement Learning.</strong></p>

<p>This blog isn’t a one stop solution and will not teach you DRL from start to finish, you will still need to learn a good portion of the curriculum from other resources. This blog is a sort of an extended cheatsheet for people to refer to when they are learning/implementing DRL via code. It contains a mix of theory and code that build on top of each other.</p>

<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
  <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em></li>
  <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em></li>
  <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</li>
  <li>Part 7 - A2C, PPO, TRPO, GAE, A3C</li>
  <li>TBA (HER, PER, Distillation)</li>
</ul>

<h2 id="sources--references">Sources &amp; References</h2>
<p>This section contains a collection of all the various sources for this blog series (in no paticular order):</p>
<ol>
  <li>Sutton, R. S., Barto, A. G. (2018 ). <a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction.</a> The MIT Press.</li>
  <li>David Silver (2015). <a href="https://www.davidsilver.uk/teaching/">Lectures on Reinforcement Learning</a></li>
  <li>Udemy Course <a href="https://www.udemy.com/course/beginner-master-rl-1/">Reinforcement Learning beginner to master - AI in Python</a></li>
  <li>Udemy Course <a href="https://www.udemy.com/course/deep-q-learning-from-paper-to-code/">Modern Reinforcement Learning: Deep Q Learning in PyTorch</a></li>
  <li>Chris G. Willcocks - Durham University <a href="https://youtube.com/playlist?list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE">Reinforcement Learning Lectures</a></li>
  <li>(My repo) <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a></li>
  <li>Pieter Abbeel <a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL</a></li>
  <li>Weng, L. (2018, February 19). <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/">A (long) peek into reinforcement learning. Lil’Log</a></li>
  <li>Aditya Chopra. (2022). <a href="https://adeecc.vercel.app/blog/intro-to-basic-rl">Introduction to Concepts in Reinforcement Learning</a></li>
</ol>

<p>This section contains a collection of various references which are required to learn DRL and have been mentioned in the curriculum but have not been covered in this blog series:</p>
<ol>
  <li>Udemy Course <a href="https://www.udemy.com/course/pytorch-for-deep-learning/">PyTorch for Deep Learning in 2023: Zero to Mastery</a></li>
  <li>Udemy Course <a href="https://www.udemy.com/course/deeplearning_x/">A deep understanding of deep learning (with Python intro)</a>
<!-- 3. Python --></li>
</ol>

<p>This section contains other references that I have not used in this blog series but are in general useful:</p>
<ol>
  <li>Coursera <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization</a></li>
  <li>HuggingFace <a href="https://huggingface.co/deep-rl-course/unit0/introduction?fw=pt">Deep Reinforcement Learning Course</a></li>
  <li>Professor Emma Brunskill, Stanford University <a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
  <li>DeepMind x UCL <a href="https://www.youtube.com/watch?v=_DpLWBG_nvk&amp;list=PLki3HkfgNEsKiZXMoYlR-14r1t_MAS7M8">RL Lecture Series</a></li>
  <li>RAIL <a href="https://www.youtube.com/watch?v=JHrlF10v2Og&amp;list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH">CS285: Deep Reinforcement Learning Series UC Berkeley</a></li>
</ol>]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Deep Reinforcement Learning" /><category term="mdp" /><category term="optimal value" /><category term="bellman" /><summary type="html"><![CDATA[Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Learning for Robotics can be broken down into three almost seperate parts: Deep Learning (Neural Networks) Reinforcement Learning Algorithm Choice of State Space, Action Space and Reward Functions]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/drl_logo.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/drl_logo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Energy Minimization based Deep Reinforcement Learning Policies for Quadrupeds</title><link href="https://textzip.github.io/posts/Energy-DRL/" rel="alternate" type="text/html" title="Energy Minimization based Deep Reinforcement Learning Policies for Quadrupeds" /><published>2022-12-11T13:13:20+05:30</published><updated>2023-02-22T10:14:52+05:30</updated><id>https://textzip.github.io/posts/Energy-DRL</id><content type="html" xml:base="https://textzip.github.io/posts/Energy-DRL/"><![CDATA[<p>The following work has been done during my time at the <a href="https://unit.aist.go.jp/jrl-22022/index_en.html">CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan</a> for my undergraduate thesis under the supervision of <a href="https://unit.aist.go.jp/jrl-22022/en/members/member-morisawa.html">Dr. Mitsuharu Morisawa</a> with support from <a href="https://unit.aist.go.jp/jrl-22022/en/members/member-singh.html">Rohan Singh</a>.</p>

<iframe width="640" height="385" src="https://youtube.com/embed/Mq8utqI5-_g" frameborder="0" allowfullscreen=""></iframe>

<blockquote>
  <p><strong>Note:</strong> Detailed results and video clips can be found in the <a href="#results">Results</a> section below.</p>
</blockquote>

<h2 id="objective">Objective</h2>
<p>The primary objective of this work was to create a deep reinforcement learning based policy for quadruped locomotion with emphasis on minimal hand tuning for deployment and easy sim-to-real transfer which was to be used as a baseline policy in our future work.</p>

<p>This has been accomplished using an energy minimization approach for the reward function along with other training specifics like curriculum learning.</p>

<h2 id="hardware--software">Hardware &amp; Software</h2>
<p>With the core belief of working towards a controller that can be deployed in real-world settings, it is extremely crucial that the entire framework of both software and hardware be scale-able and economically viable. We therefore went ahead with a commercially available quadruped
platform rather than creating our own quadrupedal platform that might make the results more difficult to verify and the solution equally harder to be deployed in a commercial scale. Similar decisions have been taken wherever crucial decisions had to be taken.</p>

<h3 id="aliengo">AlienGo</h3>
<p>The AlienGo quadrupedal platform (can be seen in the figure below) by Unitree Robotics first launched in 2018 was our choice for this research study as it strikes the perfect balance between economical cost, features and capabilities. Further, quadruped robots from Unitree Robotics have been one of the most common choice among research labs across the globe. Some select parameters of the robot are listed below:</p>
<ul>
  <li><strong>Body size</strong>: 650x310x500mm (when standing)</li>
  <li><strong>Body weight</strong>: 23kg</li>
  <li><strong>Driving method</strong>: Servo Motor</li>
  <li><strong>Degree of Freedom</strong>: 12</li>
  <li><strong>Structure/placement design</strong>: Unique design (patented)</li>
  <li><strong>Body IMU</strong>: 1 unit</li>
  <li><strong>Foot force sensor</strong>: 4 units (1 per foot)</li>
  <li><strong>Depth sensor</strong>: 2 units</li>
  <li><strong>Self-position estimation camera</strong>: 1 unit</li>
</ul>

<p><img src="/assets/img/Energy-DRL/Aliengo.jpg" alt="image1" class="shadow" /></p>

<h3 id="mujoco">MuJoCo</h3>
<p>MuJoCo has been our choice for simulation as it is a free and open source physics engine that is widely used in the industry for research and development in robotics, biomechanics, graphics and animation. Also, we have observed that it is able to simulate contact dynamics more accurately and was also faster in most of our use cases when compared to other available options. We have also used Ray an open-source compute framework for parallelization of our training.</p>

<h3 id="training-platform">Training Platform</h3>
<p>Our primary training platform is the GDEP Deep Learning BOX some select system parameters are mentioned below:</p>
<ul>
  <li><strong>CPU</strong>: AMD Ryzen Threadripper PRO 5975WX</li>
  <li><strong>Num. Cores</strong>: 32</li>
  <li><strong>RAM</strong>: 128GB</li>
  <li><strong>GPU</strong>: Nvidia RTX A6000 x 2</li>
</ul>

<h2 id="rl-framework">RL Framework</h2>
<h3 id="state-space">State Space</h3>
<p>As stated in <a href="https://arxiv.org/abs/1804.10332">Jie Tan et al. Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</a> the choice of state space has a direct impact on the sim to real transfer, we note that this can be primarily summarized as the fewer dimensions in the state space the easier it is to do a sim to real transfer as the noise and drift increase with an increase in the number of parameters being included in the state space. Many of the recent papers on quadruped locomotion therefore try to avoid using parameters that are noisy or tend to drift such as yaw from the IMU and force values from the foot sensors. While a few papers use methods like supervised learning and estimation techniques to counter the noise and drift in sensor data we decided to eliminate the use of such parameters all together as it didn’t result in any drastic change in the performance of learning policy. Our final state space has 230(46x5) dimensions and its breakdown is listed below:</p>

<ul>
  <li>$[\omega_x,\omega_y,\omega_z]$ - root angular velocity in the local frame</li>
  <li>$[\theta]$ - joint angles</li>
  <li>$[\dot{\theta}]$ - joint velocities</li>
  <li>$[c]$ - binary foot contacts</li>
  <li>$[v_x^g,v_y^g,\omega_z^g]$ - goal velocity</li>
  <li>$[a_{t-1}]$ - previous actions</li>
  <li>$[s_0,s_1,s_2,s_3]$ - history of the previous four states</li>
</ul>

<p>The goal velocity consists of three components linear velocity in x and y axis along with angular velocity along the z axis, we have discarded the roll, pitch, yaw and linear velocities that are usually included in the state space for reasons mentioned above. Further, although aliengo has force sensors that can give the magnitude of the force we decided to use a threshold and use binary representation for foot contacts as there is significant noise and drift in the readings.</p>

<h3 id="action-space">Action Space</h3>
<p>As stated in <a href="https://doi.org/10.1145%2F3099564.3099567">Michiel van de Panne et al. “Learning locomotion skills using DeepRL: does the choice of action space matter? ”</a>, the choice of action space directly effects the learning speed and hence we went ahead with joint angles as the action space representation, further to strongly center all our gait from the neutral standing pose of the robot. The policy outputs are added around the joint position values of the neutral standing pose joint angles before being fed into a low-gain PD controller that outputs the final joint torque values. The final control scheme can be seen here</p>

<p><img src="/assets/img/Energy-DRL/control_scheme.png" alt="image1" class="shadow" /></p>

<h3 id="learning-algorithm">Learning Algorithm</h3>
<p>Given the continuous nature of the state and action space Approx methods are essential. We went ahead with <a href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithm</a> that is based on the Actor-Critic Framework as it do not require extensive hyperparamter tuning and are in general is quite stable. We use a Multi Layered Perceptron architecture with 2 hidden layers of size 256 units each and ReLU activation to represent both the actor and critic networks.</p>

<h4 id="hyperparameters">Hyperparameters</h4>
<p>The hyperparameters were taken from standard implementations which are typically the same across many of the papers. The values have been listed below:</p>
<ul>
  <li><strong>Parallel Instances</strong>: 32</li>
  <li><strong>Minibatch size</strong>: 512</li>
  <li><strong>Evaluation freq</strong>: 25</li>
  <li><strong>Adam learning rate</strong>: 1e-4</li>
  <li><strong>Adam epsilon</strong>: 1e-5</li>
  <li><strong>Generalized advantage estimate discount</strong>: 0.95</li>
  <li><strong>Gamma</strong>: 0.99</li>
  <li><strong>Anneal rate for standard deviation</strong>: 1.0</li>
  <li><strong>Clipping parameter for PPO surrogate loss</strong>: 0.2</li>
  <li><strong>Epochs</strong>: 3</li>
  <li><strong>Max episode horizon</strong>: 400</li>
</ul>

<h3 id="reward-function">Reward Function</h3>
<p>The goal for any reinforcement learning policy is to maximize the total reward/expected reward collected. While the state and action space definitely effect the learning rate and stability of the policy, the reward function defines the very nature of the learning policy. The wide variety of literature available on locomotion policies for quadrupeds while almost the same with respect to
the state and action space has diversity mostly due to the choice of the reward function.</p>

\[\begin{aligned}
\text{Total Reward} &amp;= \text{Energy Cost} + \text{Survival Reward} + \text{Goal Velocity Cost} \\
\text{Energy Cost} &amp;= C_1\tau\omega \\
\text{Survival Reward} &amp;= C_2|v_x^g| + C_3|v_y^g| + C_4|\omega_z^g| \\
\text{Goal Velocity Cost} &amp;= -C_2|v_x-v_x^g| - C_3|v_y-v_y^g| - C_4|\omega_z - \omega_z^g| \\
\end{aligned}\]

<p>Where $C_1,C_2,C_3,C_4$ are constants that have to be picked.</p>

<p>We believe that the primary reason reinforcement learning based policies generalize poorly is due to the excessive number of artificial costs added to the reward function for achieving locomotion. Inspired by <a href="https://arxiv.org/abs/2111.01674">Zipeng Fu et al. Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots</a>, we base our reward function on the principle of energy minimization. Another added benefit of energy minimization based policy is the fact that different goal velocity commands result in different gaits. As explained in <a href="">Christopher L. Vaughan et al. “Froude and the contribution of naval architecture to our understanding of bipedal locomotion.”</a>, this is consistent with how animals
behave and is because a particular gait is only energy efficient for a particular range of goal velocities.</p>

<h3 id="curriculum-learning">Curriculum Learning</h3>
<p>Curriculum learning is a method of training reinforcement learning (RL) agents in which the difficulty of tasks or environments is gradually increased over time. This approach is based on the idea that starting with simpler tasks and gradually increasing the complexity can help the
agent learn more efficiently. The agent can focus on mastering basic skills needed to solve the initial tasks before attempting to tackle more complex ones. There are two ways in which curriculum learning is usually implemented. One is to use a pre-defined set of tasks or environments that are ordered by increasing difficulty. The agent is trained on these tasks in a specific order, with the difficulty of the tasks increasing as the agent progresses through the curriculum. Another approach is to use a dynamic curriculum, where the difficulty of the tasks is adjusted based on the agent’s performance. For instance, if the agent struggles with a particular task, the difficulty of that task may be reduced, while the difficulty of easier tasks may be increased to provide more challenge. We use Curriculum learning in a variety of ways to tackle a varity of issues as discussed below.</p>
<h4 id="cost-curriculum">Cost Curriculum</h4>
<p>The high energy cost with low reward for smaller values of goal velocity make it extremely difficult for the agent to learn walking at low goal speeds as it settles in a very attractive local minima of standing still and not moving at all to reduce the cost associated with energy rather
than to learn how to walk. Using a Cost Curriculum, we first let the agent walk at the required low speed with almost zero energy cost and once the agent learns a reasonable gait we slowly increase the cost to its original value so that the gait is fine tuned to be energy efficient.</p>
<h4 id="terrain-curriculum">Terrain Curriculum</h4>
<p>For increasing robustness against external perturbation and for better adaptability to real world use cases where the ground is not plane and uniform, it is useful to train the agent in uneven terrain during simulation. But introducing difficult terrain from the beginning of the training might hinder the learning and in some cases the agent might never completely solve the task as the difficulty is too high. Using terrain curriculum enables us to start with a flat plain initially
and gradually increase the difficult of the terrain to make sure the learning rate is not too difficult that the agent makes no progress at all. We train the agent across two different terrains (Plain
and Triangle) and test the learnt policy in 2 additional environments (slope and rough).
<img src="/assets/img/Energy-DRL/terrain_types.png" alt="image1" class="shadow" /></p>
<h4 id="velocity-curriculum">Velocity Curriculum</h4>
<p>Training the agent for a single goal velocity while might result in faster training speed, having the ability to smoothly transition between various goal speeds is often crucial and this is especially
important when we want the policy to adapt to any combination of linear and angular velocity given during evaluation by the user. Therefore, using a velocity curriculum enables us to randomize and cover the whole input velocity domain systematically.</p>
<h3 id="terminal-conditions">Terminal Conditions</h3>
<p>Termination conditions enable faster learning as they help the agent only explore states which are useful to the task by stopping the episode as soon as the agent reaches a state from which it cannot recover and any experience gained by the agent from that state onwards does not help
the agent learn or get better at solving the task at hand. We use two termination conditions to help increase the training speed, both of which are discussed below</p>
<h4 id="minimum-trunk-height">Minimum Trunk Height</h4>
<p>This condition ensures that the Center of Mass of the trunk is above 30cm from the ground plane as any kind of walking gait should ensure that the trunk height isn’t too low from the ground. This enable the agent to learn how to stand from a very early stage in the training
speeding up the overall learning.</p>
<h4 id="bad-contacts">Bad Contacts</h4>
<p>This condition ensures that the only points of contact the agent has with the ground plane are through the feet and no other parts of the agent are in contact with the ground plane. This minimizes the probability of the agent learning gaits or behaviours which result in collision between the agents body and the ground plane minimizing damage to agent body and other mechanical parts during deployment.</p>
<h3 id="sim-to-real">Sim to Real</h3>
<p>Sim-to-real transfer refers to the problem of transferring a reinforcement learning agent that has been trained in a simulated environment to a real-world environment. This is a challenging
problem because the simulated environment is typically different from the real-world environment in many ways, such as the dynamics of the system, the sensors and actuators, and the noise and
uncertainty present in the system.</p>

<p>We employ a mix of domain randomization and system identification for sim-to-real transfer.</p>
<h2 id="results">Results</h2>
<p>The energy minimization based policy is able to adjust its gait to the most optimal gait based on the given goal velocity. Traditional policies that do not use energy minimization are only able to exhibit a
single gait.</p>

<p>All the below videos/picture frames are from a single policy with no changes made other than the goal speed.</p>
<h3 id="gaits">Gaits</h3>
<h4 id="walk">Walk</h4>
<iframe width="640" height="385" src="https://youtube.com/embed/55T5ESUYwDY" frameborder="0" allowfullscreen=""></iframe>
<p>Generating walking gait at low-speed required the use of curriculum learning as the agent found an attractive local minima where It would just stand still without moving to avoid energy costs
at low speeds. Furthermore, curriculum learning was used as the primary sim to real transfer technique along with domain randomization.</p>

<p><img src="/assets/img/Energy-DRL/walking_gait.png" alt="image1" class="shadow" />
Terrain curriculum in particular resulted in better foot-clearance and made the agent robust to external perturbations enabling the robot to walk on extremely difficult terrain.
<img src="/assets/img/Energy-DRL/curr_example.png" alt="image1" class="shadow" /></p>

<h4 id="trot">Trot</h4>
<iframe width="640" height="385" src="https://youtube.com/embed/590fHeeqymI" frameborder="0" allowfullscreen=""></iframe>

<iframe width="640" height="385" src="https://youtube.com/embed/hgjm5DERYGM" frameborder="0" allowfullscreen=""></iframe>

<p>This is one of the most commonly generated gait for use in legged locomotion, while other methods are able to generate trotting gait we believe that our method enables us to use less
energy and torque to reach the same target velocities.
<img src="/assets/img/Energy-DRL/trotting-gait.png" alt="image1" class="shadow" /></p>

<h4 id="gallop">Gallop</h4>
<iframe width="640" height="385" src="https://youtube.com/embed/bCSGomO10ps" frameborder="0" allowfullscreen=""></iframe>

<p>While we see a Galloping gait emerge at goal speeds greater than 1.65 m/s in simulation, we need to test if the robot hardware can physically achieve this speed and therefore exhibit the
gallop gait. The other possible alternative is to use a much lower energy cost to make the agent
exhibit the gallop gait at a lower goal velocity.
<img src="/assets/img/Energy-DRL/gallop.png" alt="image1" class="shadow" /></p>

<h3 id="directional-control">Directional Control</h3>
<iframe width="640" height="385" src="https://youtube.com/embed/M02tf4fWIHI" frameborder="0" allowfullscreen=""></iframe>

<iframe width="640" height="385" src="https://youtube.com/embed/H85NNuzPzLM" frameborder="0" allowfullscreen=""></iframe>

<p>Using velocity curriculum described above, the agent is trained using a random goal velocity vector that consists of linear and angular velocity components. This enables the agent
to learn not only how to walk but also how to bank and turn in the process.
<img src="/assets/img/Energy-DRL/directional_control.png" alt="image1" class="shadow" /></p>

<h3 id="emergence-of-asymmetrical-gait">Emergence of Asymmetrical Gait</h3>
<p>Training in extremely uneven terrain leads to the emergence of asymmetrical gait that maintains
a low center of gravity and shows almost a crab like walking behaviour which is persistent even
when the policy is deployed on a smoother terrain. The results of the training are labelled as
<strong>CP2</strong> and have been described in the latter sections.
<img src="/assets/img/Energy-DRL/special_gait.png" alt="image1" class="shadow" /></p>

<h3 id="adaptation-to-unseen-terrain">Adaptation to Unseen Terrain</h3>
<p>While the agent has been trained in the triangle terrain, the learnt policy is able to successfully
walk on new terrain that it has not seen during training. The base-policy is referred to as <strong>BP</strong>,
the base policy is then subjected to two different curriculum resulting in policies <strong>CP1</strong> and <strong>CP2</strong>.
While both <strong>CP1</strong> and <strong>CP2</strong> are trained in the <strong>Triangle Terrain 1 &amp; 2</strong> the maximum height of the triangular peaks for <strong>CP2</strong> is 0.12m <strong>(Triangle Terrain 2)</strong> while it is 0.10m for <strong>CP1</strong>
<strong>(Triangle Terrain 1)</strong>.
All the three curriculum’s have been tested on unseen terrains rough with maximum peak height of 0.12m and slopes with max slope height of 0.8m and slope of 32 degrees. The results are as
follows:
<img src="/assets/img/Energy-DRL/RewardVsTimestepforPlainTerrain.svg" alt="image1" class="shadow" /></p>

<p><img src="/assets/img/Energy-DRL/RewardVsTimestepforRoughTerrain.svg" alt="image1" class="shadow" /></p>

<p><img src="/assets/img/Energy-DRL/RewardVsTimestepforSlopeTerrain.svg" alt="image1" class="shadow" /></p>

<p><img src="/assets/img/Energy-DRL/RewardVsTimestepforTriangleTerrain1.svg" alt="image1" class="shadow" /></p>

<p><img src="/assets/img/Energy-DRL/RewardVsTimestepforTriangleTerrain2.svg" alt="image1" class="shadow" /></p>

<h3 id="inference">Inference</h3>
<p>While <strong>CP2</strong> has a clear advantage when deployed in <strong>Triangle Terrain 2</strong>, it performs almost as good as or slightly worse than <strong>CP1</strong> in all the other test cases. Furthermore, it is clearly visible that <strong>BP</strong> isn’t suitable for most of the testing environment as it flat-lines pretty early. While <strong>CP2</strong> learns a more stable gait pattern it is slower and requires lot more movement by the agent which results in <strong>CP1</strong> gaining a few points over it as <strong>CP1</strong> can quickly cover the velocity cost.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Most of the current reinforcement learning based policies use highly constrained rewards due to
which the locomotion policy developed doesn’t use the entire solution space. Energy minimization
based policy is able to adjust its gait to the most optimal gait based on the goal velocity.
Traditional policies that do not use energy minimization are only able to exhibit a single gait.</p>
<h3 id="current-limitations">Current Limitations</h3>
<p>The current implementation of the policy although exhibits different gaits when trained at
different goal velocities, it fails to learn more than one gait during a single training run. We
believe this is due to the difference in the weights of the network for different gaits. Also, while
training in extremely unstructured environments leads to the emergence of asymmetrical gait
that is extremely stable, the policy seems to forget the older gait and tends to use this gait even
when deployed later on plain terrain</p>

<!-- Link for Thesis Report: https://drive.google.com/file/d/1Iqkzg1Hm_KEukPT_LOcBMrR1BnOSRvw4/view?usp=sharing -->]]></content><author><name>Jai Krishna</name></author><category term="Projects" /><category term="Quadrupeds" /><category term="mdp" /><category term="optimal value" /><category term="bellman" /><summary type="html"><![CDATA[The following work has been done during my time at the CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan for my undergraduate thesis under the supervision of Dr. Mitsuharu Morisawa with support from Rohan Singh.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/Energy-DRL/go1.png" /><media:content medium="image" url="https://textzip.github.io/assets/img/Energy-DRL/go1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning - Part 2 - Finite MDP</title><link href="https://textzip.github.io/posts/DRL-2/" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Part 2 - Finite MDP" /><published>2022-11-16T13:13:20+05:30</published><updated>2023-01-23T16:42:20+05:30</updated><id>https://textzip.github.io/posts/DRL-2</id><content type="html" xml:base="https://textzip.github.io/posts/DRL-2/"><![CDATA[<p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>

<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
  <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em></li>
  <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em></li>
  <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</li>
  <li>Part 7 - A2C, PPO, TRPO, GAE, A3C</li>
  <li>TBA (HER, PER, Distillation)</li>
</ul>

<h1 id="finite-mdp">Finite MDP</h1>
<p>We will now consider problems that involve evaluative
feedback, as in bandits, but also an associative aspect—choosing different actions in different situations.</p>

<p>MDPs are a classical formalization of sequential decision making,
where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to trade off immediate and delayed reward.</p>

<p>In bandit problems we estimated the value $q_{\star}(a)$ of each action $a$, in MDPs we estimate the value $q_\star(s, a)$ of each action $a$ in each state $s$, or we estimate the value $v_\star(s)$ of each state given optimal action selections. These state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections.</p>

<h2 id="mdp-framework">MDP Framework</h2>
<p><img src="/assets/img/FiniteMDP/AgentWorld.png" alt="image1" class="shadow" />
In the above figure, the Agent is the learner and decision maker. Everything other than the agent is called the environmnet.</p>

<p>The agent and the environmnet interact with each other at every discrete timestep $t=0,1,2,..$ at each timestep the agent receives some representation of the environment’s state $S_t \in \mathit{S}$, and on that basis selects an action $A_t \in \mathit{A}(s)$ and one timestep later the environment responds to this action and presents a new state $S_{t+1}$ to the agent along with a numerical value called <em>reward</em> $R_{t+1} \in  \mathit{R} \subset \mathbb{R}$ which the agent seeks to maximize over time through its choice of actions.</p>

<p>Taking the Cliff-Walking environment as an example, here is how we can access information about the state and observation space in gymnasium.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CliffWalking-v0</span><span class="sh">"</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>

<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The initial state after reset is : </span><span class="si">{</span><span class="n">observation</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The State space is of the type: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The Action space is of the type: </span><span class="si">{</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The output for the above program is as follows:</p>
<div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="go">The initial state after reset is : 36
The State space is of the type: Discrete(48)
The Action space is of the type: Discrete(4)
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:</p>

\[S_0,A_0,R_1,S_1,A_1,R_2,...\]

<p>The following is the python implementation for generating a trajectory for N=5 steps in the Cliff Walking Env
<img src="/assets/img/DRL2/cliff_walking.gif" alt="image1" class="shadow" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CliffWalking-v0</span><span class="sh">"</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>

<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">trajectory</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>
    <span class="n">new_observation</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
    <span class="n">trajectory</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">observation</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
    <span class="n">observation</span><span class="o">=</span><span class="n">new_observation</span>
    
    <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The Trajectory for 5 steps is: </span><span class="si">{</span><span class="n">trajectory</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The output of the above program is as follows:</p>
<div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="go">The Trajectory for 5 steps is: [[36, 3, -1], [36, 3, -1], [36, 3, -1], [36, 2, -1], [36, 0, -1]]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>The above code can be slightly modified to generate the trajectory for an entire episode as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CliffWalking-v0</span><span class="sh">"</span><span class="p">,</span><span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">observation</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">trajectory</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>
    <span class="n">new_observation</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
    <span class="n">trajectory</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">observation</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">new_observation</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trajectory for entire episode: </span><span class="si">{</span><span class="n">trajectory</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>In a ﬁnite MDP, the sets of states, actions, and rewards ($\mathit{S}$, $\mathit{A}$, and $\mathit{R}$) all have a ﬁnite number of elements. In this case, the random variables $R_t$ and $S_t$ have well deﬁned
discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s’ \in \mathit{S}$ and $r \in \mathit{R}$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:</p>

\[p(s',r|s,a) \ \dot{=} \ \text{Pr}\{S_t=s',R_t=r \ | \  S_{t-1}=s,A_{t-1}=a \}\]

<p>Further since p speciﬁes a probability distribution for each choice of s
and a,</p>

\[\sum_{s'\in \mathit{S}}\sum_{r \in \mathit{R}}p(s',r|s,a)=1, \text{for all } s \in \mathit{S}, a \in \mathit{A}(s)\]

<p>In a Markov decision process, the probabilities given by $p$ completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $R_t$ depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$, and, given them,
not at all on earlier states and actions. Therefore the state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it
does, then the state is said to have the Markov property.</p>

<p>From the four-argument dynamics function, $p$, one can compute anything else one might
want to know about the environment, such as</p>

<h3 id="state-transition-probabilities">State-Transition Probabilities</h3>
<p>A three-argument function denoting the probability of reaching state $s’$ when action $a$ is taken from state $s$.</p>

\[p(s'|s,a) \ \dot{=} \ \text{Pr}\{S_t=s'|S_{t-1}=s,A_{t-1}=a \} = \sum_{r \in \mathit{R}}p(s',r|s,a)\]

<h3 id="expected-reward-for-state-action">Expected Reward for State-Action</h3>
<p>A two-argument function defining the expected rewards for a state-action pair.</p>

\[r(s,a) \ \dot{=} \ \mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a] = \sum_{r \in \mathit{R}} \left[ r\sum_{s' \in \mathit{S}} p(s',r|s,a)\right]\]

<h3 id="expected-reward-for-state-action-state">Expected Reward for State-Action-State</h3>
<p>A Three argument function defining the expected rewards for a state-action-state pair.</p>

\[\begin{align*}
r(s,a,s') \ \dot{=} \ \mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'] &amp;= \sum_{r \in \mathit{R}}r\dfrac{p(s',r|s,a)}{p(s'|s,a)} \\
&amp;= \sum_{r \in \mathit{R}}r\dfrac{p(s'|s,a).p(r|s,a,s')}{p(s'|s,a)} \\
&amp;= \sum_{r \in \mathit{R}}r.p(r|s,a,s') \\ 
\end{align*}\]

<h2 id="goals--rewards">Goals &amp; Rewards</h2>
<p>In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent. At each time step, the reward is a simple number, $R_t \in \mathit{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run.</p>

<p>The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved..</p>

<blockquote>
  <p><strong>The Reward Hypothesis</strong>
That all of what we mean by goals and purposes can be well thought of as
the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>

<h2 id="returns--episodes">Returns &amp; Episodes</h2>
<p>To formally define how we wish to maximize the cumulative reward, we seek to maximize the expected return, where the return, denoted $G_t$,is
deﬁned as some speciﬁc function of the reward sequence. In the simplest case the return is the sum of the rewards:</p>

\[G_t \ \dot{=}\ R_{t+1} + R_{t+2} + R_{t+3}+...+R_T\]

<p>Where T is the final timestep, this approach is applicable when there is a natural notion of ﬁnal time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a
sample from a standard distribution of starting states. Even if you think of episodes as ending in different ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to
end in the same terminal state, with different rewards for the different outcomes.</p>

<p>Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $S$, from the set of all states plus
the terminal state, denoted $S^+$. The time of termination, $T$, is a random variable that normally varies from episode to episode.</p>

<p>On the other hand, in many cases the agent–environment interaction does not break naturally into identiﬁable episodes, but goes on continually without limit. For example,this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation discussed above is problematic for continuing tasks because the ﬁnal time step would be $T = \infty$, and the return, which is what we are trying to maximize, could easily be inﬁnite.</p>

<p>The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted
return:</p>

\[G_t \ \dot{=}\ R_{t+1} + \gamma{R_{t+2}} + \gamma^2{R_{t+3}}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\]

<p>Where $\gamma$ is called the discount factor and lies between $[0,1]$. The above formula can also be written recursivly as,</p>

\[G_t \ \dot{=} R_{t+1} + \gamma G_{t+1}\]

<h2 id="unified-notation-for-tasks">Unified Notation for Tasks</h2>
<p>We need one other convention to obtain a single notation that covers both episodic and continuing tasks. We have deﬁned the return as a sum over a ﬁnite number of terms in one case and as a sum over an inﬁnite number of terms in the other. These two can be uniﬁed by considering episode termination to be the entering of a special absorbing state that transitions only to itself and that generates only rewards of zero. For example, consider the state transition diagram:
<img src="/assets/img/FiniteMDP/UnifiedTask.png" alt="image1" class="shadow" />
Here the solid square represents the special absorbing state corresponding to the end of an
episode. Starting from $S_0$, we get the reward sequence +1, +1, +1, 0, 0, 0,. …Summing these, we get the same return whether we sum over the ﬁrst $T$ rewards (here $T$ = 3) or over the full inﬁnite sequence. This remains true even if we introduce discounting.</p>

<p>We can therefore write the expected return as,</p>

\[G_{t}  \dot{=}  \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k\]

<p>Where there is a possibilty that $T = \infty$ or $\gamma = 1$ (but not both).</p>

<p>The above formula can be used in code as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span> 

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">CliffWalking-v0</span><span class="sh">"</span><span class="p">,</span><span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">G</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Return 
</span><span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>
    <span class="n">observation</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
    <span class="n">G</span> <span class="o">+=</span> <span class="n">reward</span><span class="o">*</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">counter</span><span class="p">)</span>
    <span class="n">counter</span> <span class="o">+=</span><span class="mi">1</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The episode terminated after </span><span class="si">{</span><span class="n">counter</span><span class="si">}</span><span class="s"> steps with Return(G) </span><span class="si">{</span><span class="n">G</span><span class="si">}</span><span class="s"> for gamma </span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The output for the above code with various values of gamma is as follows:</p>
<div class="language-terminal highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="go">The episode terminated after 853 steps with Return(G) -8575 for gamma 1

The episode terminated after 1391 steps with Return(G) -1445.9837656301004 for gamma 0.99

The episode terminated after 4516 steps with Return(G) -1 for gamma 0
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="policies--value-functions">Policies &amp; Value Functions</h2>
<p>Almost all reinforcement learning algorithms involve estimating value functions— which are functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p>

<p>The notion of “how good” here is deﬁned in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions
are deﬁned with respect to particular ways of acting, called policies.</p>

<p>Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\pi$ at time $t$,then $\pi(a\mid s)$ is the probability that $A_t = a$ if $S_t = s$. Like $p,\pi$ is an ordinary function.</p>

<p>The $\mid$ in the middle of $\pi(a\mid s)$ merely reminds us that it deﬁnes a probability distribution over $a \in \mathit{A}(s)$ for each $s \in \mathit{S}$. Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.</p>

<h3 id="state-value-function">State Value Function</h3>
<p>The value function of a state $s$ under a policy $\pi$, denoted by $v_\pi(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter. For MDPs, we can deﬁne $v_\pi$ formally as</p>

\[\begin{align*}
v_\pi(s) \ &amp;\dot{=} \ \mathbb{E}_\pi[G_t|S_t=s] = \mathbb{E}\left[ \sum_{k=0}^{\infty}\gamma^k R_{k+t+1} | S_t=s\right] \text{ for all } s \in \mathit{S}\\
v_\pi(s) \ &amp;\dot{=} \ \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s]\\
v_\pi(s) \ &amp;\dot{=} \ \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi({S_{t+1}})|S_t=s]
\end{align*}\]

<p>where $\mathbb{E}[.]$ denotes the expected value of the variable given that the agent follows the policy $\pi$. Also, the value of the terminal state is considered to be zero.</p>

<h3 id="action-value-function">Action Value Function</h3>
<p>The value of taking action $a$ in state $s$ under a policy $\pi$, denoted by $q_\pi(s, a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>

\[\begin{align*}
q_\pi(s,a) \ &amp;\dot{=} \ \mathbb{E}_\pi[G_t|S_t=s,A_t=a] = \mathbb{E}\left[ \sum_{k=0}^{\infty}\gamma^k R_{k+t+1} | S_t=s, A_t=a\right]\\
q_\pi(s,a) \ &amp;\dot{=} \ \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s,A_t=a]\\
q_\pi(s,a) \ &amp;\dot{=} \ \mathbb{E}_\pi[R_{t+1} + \gamma q_\pi{(S_{t+1})}|S_t=s,A_t=a]\\
\end{align*}\]

<h3 id="bellman-value-equations">Bellman Value Equations</h3>

<p><img src="/assets/img/FiniteMDP/v_q.png" alt="image1" class="shadow" /></p>

\[v_\pi(s) = \sum_a \pi(a\mid s)q_\pi(s,a) \tag{2.1}\]

<p><img src="/assets/img/FiniteMDP/q_v.png" alt="image1" class="shadow" /></p>

\[q_\pi(s,a) = \sum_{s',r} p(s',r\mid s,a)(r + \gamma v_\pi(s')) \tag{2.2}\]

<p><img src="/assets/img/FiniteMDP/v_q_v.png" alt="image1" class="shadow" /></p>

<p>Substuting equation (2.2) in (2.1) we get the following recursive relation in terms of $v_\pi.$</p>

\[v_\pi(s) = \sum_{a}\pi(a\mid s)\sum_{s',r} p(s',r\mid a,s)[r+\gamma v_\pi(s')] \tag{2.2}\]

<p><img src="/assets/img/FiniteMDP/q_v_q.png" alt="image1" class="shadow" /></p>

<p>Substuting equation (2.1) in (2.2) we get the following recursive relation in terms of $q_\pi.$</p>

\[q_\pi(a,s) = \sum_{s',r}p(s',r\mid s,a)[r + \gamma \sum_a' \pi(a'\mid s')q_\pi(s',a')] \tag{2.3}\]

<h2 id="optimal-policies--value-functions">Optimal Policies &amp; Value Functions</h2>
<p>There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal
state-value function, denoted $v_\star$.</p>

<p><img src="/assets/img/FiniteMDP/v_q_o.png" alt="image1" class="shadow" /></p>

\[v_\star(s) = \text{max}_a q_\star (s,a) \tag{2.4}\]

<p><img src="/assets/img/FiniteMDP/q_v_o.png" alt="image1" class="shadow" /></p>

\[q_\star(s,a) = \text{max}\sum_{s',r} p(s',r\mid s,a)(r + v_\star(s')) \tag{2.5}\]

<p><img src="/assets/img/FiniteMDP/v_q_v_o.png" alt="image1" class="shadow" /></p>

<p>Substuting equation (2.5) in (2.4) we get the following recursive relation in terms of $v_\pi.$</p>

\[v_\star(s) = \text{max}\sum_{s',a}p(r,s'\mid s,a)[r + \gamma v_\star (s')] \tag{2.6}\]

<p><img src="/assets/img/FiniteMDP/q_v_q_o.png" alt="image1" class="shadow" /></p>

<p>Substuting equation (2.4) in (2.5) we get the following recursive relation in terms of $q_\pi.$</p>

\[q_\star(s,a) = \sum p(s',r\mid s,a) [r + \gamma\  \text{max} \ q_\star(s',a')] \tag{2.7}\]

<!-- TODO Add equations for expressing the policy in terms of the value functions -->]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Deep Reinforcement Learning" /><category term="mdp" /><category term="optimal value" /><category term="bellman" /><summary type="html"><![CDATA[This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/drl_logo.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/drl_logo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits</title><link href="https://textzip.github.io/posts/DRL-1/" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits" /><published>2022-11-15T19:13:20+05:30</published><updated>2023-01-23T16:42:20+05:30</updated><id>https://textzip.github.io/posts/DRL-1</id><content type="html" xml:base="https://textzip.github.io/posts/DRL-1/"><![CDATA[<p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>

<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
  <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a></li>
  <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em></li>
  <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em></li>
  <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em></li>
  <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC</li>
  <li>Part 7 - A2C, PPO, TRPO, GAE, A3C</li>
  <li>TBA (HER, PER, Distillation)</li>
</ul>

<p>Reinforcement learning loosely refers to the area of machine learning where an agent is tasked with learning about the concequences of its actions and in that process also maximize the numerical reward signal collected over time.</p>

<h1 id="k-arm-bandits">K-Arm Bandits</h1>
<p>In this simplified setting we assume that the problem is non-associatve and therefore does not involve learning to act in more than one situation.</p>

<blockquote>
  <p>Consider the following problem, You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
</blockquote>

<p>Each of the actions has an expected/mean reward associated with it, called the <em>value</em> of the action ($A_t$). At any timestep $t$ the reward for choosing a paticular action is denoted by $R_t$. The value of an arbitrary action can then be represented as:</p>

\[q_*(a) \ \dot{=} \ \mathbb{E}[R_t|A_t = a ]\]

<p><em>The value of the action is defined as the expected reward associated with choosing the action.</em></p>

<p>Given the values of all actions it is quite trivial to solve the k-arm bandit problem as one would choose the action with the highest value all the time. But in most close-to-real life senarios the action values are not given and need to be estimated first. Let $Q_t(a)$ denote the estimated value of the action $a$ at time $t$.</p>

<p>We have two possible moves here,</p>
<ul>
  <li>Make our estimates better by sampling all actions aka <strong>Exploration</strong></li>
  <li>Choose the highest action value given the current estimates aka <strong>Exploit</strong></li>
</ul>

<p>The need to balance <strong>exploration</strong> and <strong>exploitation</strong> is a distinctive challenge that arises in reinforcement learning. Below mentioned are a few methods to do so..</p>
<h2 id="action-value-methods">Action-value Methods</h2>
<p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p>
<h3 id="sample-average">Sample Average</h3>
<p>Recall that the value of an action is the mean reward recieved when the action is choosen, one approach therefore is to average all the rewards for an action. Given by</p>

\[Q_{n+1}(a) \ \dot{=} \ \dfrac{R_1+R_2+..+R_{n}}{n}\]

<p><em>The estimated value of an action after being selected n times</em>
For easier implementation can be written as a recursive formula as follows:</p>

\[\begin{align*}
Q_{n+1}(a) &amp;= \dfrac{1}{n} \left( \sum_{i=1}^{i=n}{R_i} \right)\\ 
 &amp;= \dfrac{1}{n} \left( R_n + (n-1)\dfrac{1}{(n-1)}\sum_{i=1}^{i=n-1}{R_i} \right)\\
&amp;= \dfrac{1}{n} \left( R_n + (n-1)Q_n(a) \right)\\
&amp;= \dfrac{1}{n} \left( R_n + nQ_n(a) - Q_n(a) \right)\\
&amp;= Q_n(a) + \dfrac{1}{n}\left[ R_n - Q_n(a)\right]
\end{align*}\]

<p>The above update rule can be summarised as follows:</p>

\[NewEstimate = OldEstimate + stepSize[Target-OldEstimate]\]

<p>A few observations based on the above:</p>
<ul>
  <li>$[Target-OldEstimate]$ is the error in the estimate that we wish to minimize by taking a step towards the target.</li>
  <li>The stepSize parameter is not constant (For e.g. in sample avg it was 1/n) and is often denoted by $\alpha_t(a)$</li>
</ul>

<p>Given the estimates we can now focus on exploitation, a simple rather “greedy” method would be to choose the highest action value which can be represented as:</p>

\[A_t = \text{argmax}_{a}Q_t(a)\]

<p>The expression $\text{argmax}_{a}$ denotes that a is choosen such that $Q_t(a)$ is maximised with ties broken arbitrarily.</p>

<p>The above method of action selection is quite weak as it does no exploration and therefore can be acting on false assumtions of the action values. A better approach would be to exploit most of the time but every now and then explore values as well. This is called the $\varepsilon$-greedy methods where the decision to explore is based on a small probability $\varepsilon$.</p>

<h3 id="exponential-recency-weighted-average">Exponential Recency-Weighted Average</h3>
<p>The combination of $\varepsilon$-greedy with sample averages works well for stationary problems where the reward probabilties do not change with time. But as stated at the start in most cases this assumption isn’t valid. Therefore the solution for problems that involve non-statinoary reward distributions is to give more weight to recent rewards when compared to old rewards and one possible way to achieve this is to use a constant stepSize parameter that lies between $[0,1]$.</p>

<p>The estimated value from sample value method can be re-written as follows:</p>

\[\begin{align*}
Q_{n+1}(a) &amp;= Q_n(a) + \alpha\left[ R_n - Q_n(a)\right]\\
&amp;= \alpha{R_n}+ (1-\alpha)Q_n\\
&amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}\]

<p>The final step can be derived based on the derivation in the sample based method section and therefore has been cut-short.</p>

<p>The above is called weighted-average because the sum of weights is equal to 1, infact the weights decrease exponentially and the above method is therefore called Exponential Recency-Weighted Average.</p>

<h3 id="initial-values--bias">Initial Values &amp; Bias</h3>
<p>When an action is being selected for the first time in sample average method the denominator is 0, therfore a default value is assumed in such cases and in Exponential Recency-Weighted Average method, the value of $Q_2$ depends on the assumption $Q_1$ which again has to be assumed.</p>

<p>Therefore these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant stepSize, the bias is permanent, though decreasing over time as seen in the equation derived above.</p>

<p>In practice this kind of bias is helpful when the initial values choosen are based on expert knowledge. Initial values can also be used to encourage exploration, instead of setting the initial values to 0 if we set them to a very high value aka “optimistic value” the agent tries each action and gets dissapointed but keeps on trying until all the values are sampled atleast once this method often reffered to as optimistic initial value makes sure there is exploration at the start even when used with pure greedy methods.</p>

<h3 id="exponential-recency-weighted-average-without-initial-bias">Exponential Recency-Weighted Average without Initial Bias</h3>
<p>Given that the Exponential Recency-Weighted Average while works on non-stationary problems but suffers from initial bias and the sample average methods are less effected by the initial bias but are not effective against non-stationary problems, there is a need for another method that can work well with non-stationary problems without any intial bias.</p>

<p>One such method can be formulated with the use of a new stepSize parameter defined as</p>

\[\beta_n \ \dot{=} \ \dfrac{\alpha}{\bar{o}_n}\]

<p>To process the nth reward for a particular action, where $\alpha$&lt;0 is the conventional stepSize and $\bar{o}_n$ is the trace of one that starts at 0:</p>

<p>\(\bar{o}_n = \bar{o}_{n-1} + \alpha(1-\bar{o}_{n-1}), \text{ for } n&gt;0, \text{ with } \bar{o}_0 \ \dot{=} \ 0\)</p>
<h3 id="upper-conﬁdence-bound">Upper-Conﬁdence-Bound</h3>
<p>In the $\varepsilon$-greedy methods, while randomly picking actions every once in a while to encourage exploration helps, it makes much more sense to pick these actions based on some guided hurestic rather than random-sampling.</p>

<p>One possible alternative is to pick among the non-greedy actions is to opt for actons that have higher degree of uncertainty in their estimates, such a mechanism will therefore allow us to get better overall estimates for all action values.</p>

<p>The idea of upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a,with
c determining the conﬁdence level.</p>

<p>The action is therefore selected as follows:</p>

\[A_t \ \dot{=} \ \text{argmax}_a \left[Q_t(a) + c\sqrt{\dfrac{ln\ t}{N_t(a)}} \right]\]

<p>Where, $N_t(a)$ denotes the number of times action $a$ has been selected prior to time $t$ and $c$ &gt;0 controls the degree of exploration. When an action is being explored for the first time it is considered to be a maximizing action.</p>

<p>Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than $a$ is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases.</p>

<p>The use of the natural logarithm means that the increases get smaller over time, but are
unbounded; all actions will eventually be selected, but actions with lower value estimates,
or that have already been selected frequently, will be selected with decreasing frequency
over time.</p>
<h3 id="gradient-bandit">Gradient-Bandit</h3>
<p>So far we have considered methods that estimate action values and use
those estimates to select actions. This is often a good approach, but it is not the only one possible.</p>

<p>In this section we consider learning a numerical preference for each action $a$, which we denote as $H_t(a) \in \mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative
preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according
to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:</p>

\[\text{Pr}\{A_t=a\} \ \dot{=} \ \dfrac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}  \ \dot{=} \ \pi_t(a)\]

<p>Where $\pi_t(a)$ is the probability of choosing action $a$ at time $t$. Initially all actions have the same preference, therefore $H_1(a)=0$.</p>

<p>There is a natural learning algorithm for soft-max action preferences based on the idea
of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the
reward $R_t$, the action preferences are updated by:</p>

\[\begin{align*}
H_{t+1}(A_t) \ &amp;\dot{=} \ H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) \\
H_{t+1}(a) \ &amp;\dot{=} \ H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) \text{ for all } a\neq A_t \\
\end{align*}\]

<p>where $\alpha &gt; 0$ is a step-size parameter, and $\bar{R}_t \in \mathbb{R}$ is the average of the rewards up to but
not including time t (with $\bar{R}_1 = R_1$), which can be computed incrementally.  The $\bar{R}_t$ term serves as a
baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite
direction.</p>

<h2 id="convergence-conditions">Convergence Conditions</h2>
<p>Not all stepSize values guarentee that the expected action value converges to the true values. A well-known result in stochastic approximation theory gives us the conditions required to
assure convergence with probability 1:</p>
<ul>
  <li>The ﬁrst condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random ﬂuctuations.</li>
  <li>The second condition guarantees that eventually the steps become small enough to assure convergence.</li>
</ul>

<p>These can be expressed as follows:</p>

\[\begin{align*}
\sum_{n=1}^{\infty}\alpha_n(a) &amp;= \infty\\
\sum_{n=1}^{\infty}\alpha_n^2(a) &amp;&lt; \infty\\
\end{align*}\]

<h2 id="associative-search">Associative Search</h2>
<p>In the above discussed methods the learner either tries to ﬁnd a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we brieﬂy discuss the simplest way in which nonassociative tasks extend to the associative setting.</p>

<p>As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you
do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.
This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best.</p>

<p>Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.</p>

<!-- TO-DO https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%202-%20Exploration%20and%20control_slides.pdf  -->]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Deep Reinforcement Learning" /><category term="bandits" /><category term="sample average" /><category term="weighted average" /><category term="ucb" /><category term="bias" /><summary type="html"><![CDATA[This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/drl_logo.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/drl_logo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Autonomus Ground Vehicle</title><link href="https://textzip.github.io/posts/AGV/" rel="alternate" type="text/html" title="Autonomus Ground Vehicle" /><published>2022-01-05T19:13:20+05:30</published><updated>2022-01-05T19:13:20+05:30</updated><id>https://textzip.github.io/posts/AGV</id><content type="html" xml:base="https://textzip.github.io/posts/AGV/"><![CDATA[<p><img src="/assets/img/AGV/proto_02.jpg" alt="Image1" class="shadow" /></p>

<p>An Autonomous Ground Vechile for exploring concepts like localization, path planning, control in ROS.</p>

<h1 id="hardware">Hardware</h1>
<ul>
  <li>NVIDIA Jetson Nano</li>
  <li>Arduino Mega</li>
  <li>Buck Converter</li>
  <li>RPLIDAR A8</li>
  <li>L298N Motor Driver Module x2</li>
  <li>100 RPM Geared Motors + Encoders x4</li>
  <li>2200 mAh 3S 35C Lipo x1</li>
</ul>

<h2 id="layout">Layout</h2>
<p><img src="/assets/img/AGV/chart_white.png" alt="Image1" class="shadow" /></p>
<ul>
  <li>Solid bi-directional green lines represent lines that transfer both power and data.</li>
  <li>Dark red lines represent 12V power transfer lines</li>
  <li>Light red lines represent 5V power transfer lines</li>
  <li>dashed blue lines represent data transfer lines</li>
  <li>Red squares represnt power supply/distribution components</li>
  <li>Blue squares represent inputs/sensors</li>
  <li>Yellow squares represent computational units</li>
  <li>Purple squares represent output units/actuators.</li>
</ul>

<h1 id="results">Results</h1>
<p>Preliminary scan data
<img src="/assets/img/AGV/rviz_1.png" alt="Image1" class="shadow" />
<img src="/assets/img/AGV/proto_01.jpg" alt="Image1" class="shadow" /></p>

<p>This is an ongoing project, documentation will be updated soon.</p>

<h1 id="ros-node-tree">ROS Node Tree</h1>
<h1 id="slam">SLAM</h1>
<h1 id="path-planning">Path Planning</h1>]]></content><author><name>Jai Krishna</name></author><category term="Projects" /><category term="Autonomous Robots" /><category term="ws2812b" /><category term="esp32" /><category term="mic" /><category term="bluetooth" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/AGV/proto_02.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/AGV/proto_02.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">URDF</title><link href="https://textzip.github.io/posts/ROS-URDF/" rel="alternate" type="text/html" title="URDF" /><published>2022-01-02T19:13:20+05:30</published><updated>2022-01-18T16:36:00+05:30</updated><id>https://textzip.github.io/posts/ROS-URDF</id><content type="html" xml:base="https://textzip.github.io/posts/ROS-URDF/"><![CDATA[<!-- ![Image1](/assets/img/LiteBar/closeup.jpg){: .shadow} -->

<p>The URDF file format is a way to represent robots and their components in ROS. It uses a modified verion of the XML syntax.</p>

<h1 id="structure-of-urdf">Structure of URDF</h1>

<p><img src="/assets/img/URDF/URDF_chart.png" alt="Image1" class="shadow" /></p>

<!-- Insert the huge ass flowchart here -->

<h2 id="geometry-types">geometry types</h2>

<h3 id="box">box</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;box</span> <span class="na">size=</span><span class="s">"0.6 0.1 0.2"</span><span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>size attribute contains the three side lengths of the box. The origin of the box is in its center.
    <h3 id="cylinder">cylinder</h3>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;cylinder</span> <span class="na">length=</span><span class="s">"0.6"</span> <span class="na">radius=</span><span class="s">"0.2"</span><span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>Specify the radius and length. The origin of the cylinder is in its center.
    <h3 id="sphere">sphere</h3>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;sphere</span> <span class="na">radius=</span><span class="s">"3.0"</span><span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li>Specify the radius. The origin of the sphere is in its center.
    <h3 id="mesh">mesh</h3>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;mesh</span> <span class="na">filename=</span><span class="s">"package://robot_description/meshes/base_link_simple.DAE"</span><span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li>A trimesh element specified by a filename, and an optional scale that scales the mesh’s axis-aligned-bounding-box. Any geometry format is acceptable but specific application compatibility is dependent on implementation.</li>
  <li>The recommended format for best texture and color support is Collada .dae files. The mesh file is not transferred between machines referencing the same model. It must be a local file. Prefix the filename with package://<packagename>/<path> to make the path to the mesh file relative to the package <packagename>.
</packagename></path></packagename>    <h2 id="joint-types">joint types</h2>
    <h3 id="fixed">fixed</h3>
    <div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"base_to_right_leg"</span> <span class="na">type=</span><span class="s">"fixed"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"base_link"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"right_leg"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 -0.22 0.25"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/joint&gt;</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ul>

<h3 id="continuous">continuous</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"head_swivel"</span> <span class="na">type=</span><span class="s">"continuous"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"base_link"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"head"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;axis</span> <span class="na">xyz=</span><span class="s">"0 0 1"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 0 0.3"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The connection between the body and the head is a continuous joint, meaning that it can take on any angle from negative infinity to positive infinity. The wheels are also modeled like this, so that they can roll in both directions forever.</p>

<p>The only additional information we have to add is the axis of rotation, here specified by an xyz triplet, which specifies a vector around which the head will rotate. Since we want it to go around the z axis, we specify the vector “0 0 1”.</p>

<h3 id="revolute">revolute</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"left_gripper_joint"</span> <span class="na">type=</span><span class="s">"revolute"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;axis</span> <span class="na">xyz=</span><span class="s">"0 0 1"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;limit</span> <span class="na">effort=</span><span class="s">"1000.0"</span> <span class="na">lower=</span><span class="s">"0.0"</span> <span class="na">upper=</span><span class="s">"0.548"</span> <span class="na">velocity=</span><span class="s">"0.5"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span> <span class="na">xyz=</span><span class="s">"0.2 0.01 0"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"gripper_pole"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"left_gripper"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Both the right and the left gripper joints are modeled as revolute joints. This means that they rotate in the same way that the continuous joints do, but they have strict limits. Hence, we must include the limit tag specifying the upper and lower limits of the joint (in radians). We also must specify a maximum velocity and effort for this joint but the actual values don’t matter for our purposes here.</p>

<h3 id="prismatic">prismatic</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"gripper_extension"</span> <span class="na">type=</span><span class="s">"prismatic"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"base_link"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"gripper_pole"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;limit</span> <span class="na">effort=</span><span class="s">"1000.0"</span> <span class="na">lower=</span><span class="s">"-0.38"</span> <span class="na">upper=</span><span class="s">"0"</span> <span class="na">velocity=</span><span class="s">"0.5"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span> <span class="na">xyz=</span><span class="s">"0.19 0 0.2"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The gripper arm is a different kind of joint, namely a prismatic joint. This means that it moves along an axis, not around it. This translational movement is what allows our robot model to extend and retract its gripper arm.</p>

<p>The limits of the prismatic arm are specified in the same way as a revolute joint, except that the units are meters, not radians.</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="rouge-code"><pre><span class="cp">&lt;?xml version="1.0"?&gt;</span>
<span class="nt">&lt;robot</span> <span class="na">name=</span><span class="s">"multipleshapes"</span><span class="nt">&gt;</span>
	<span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"dummy"</span><span class="nt">&gt;</span>
	<span class="nt">&lt;/link&gt;</span>
  <span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"base_link"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;visual&gt;</span>
      <span class="nt">&lt;geometry&gt;</span>
        <span class="nt">&lt;cylinder</span> <span class="na">length=</span><span class="s">"0.6"</span> <span class="na">radius=</span><span class="s">"0.2"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;/geometry&gt;</span>
      <span class="nt">&lt;material</span> <span class="na">name=</span><span class="s">"blue"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;color</span> <span class="na">rgba=</span><span class="s">"0 0 .8 1"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;/material&gt;</span>
    <span class="nt">&lt;/visual&gt;</span>
    <span class="nt">&lt;collision&gt;</span>
      <span class="nt">&lt;geometry&gt;</span>
        <span class="nt">&lt;cylinder</span> <span class="na">length=</span><span class="s">"0.6"</span> <span class="na">radius=</span><span class="s">"0.2"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;/geometry&gt;</span>
    <span class="nt">&lt;/collision&gt;</span>
    <span class="nt">&lt;inertial&gt;</span>
      <span class="nt">&lt;mass</span> <span class="na">value=</span><span class="s">"10"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;inertia</span> <span class="na">ixx=</span><span class="s">"0.4"</span> <span class="na">ixy=</span><span class="s">"0.0"</span> <span class="na">ixz=</span><span class="s">"0.0"</span> <span class="na">iyy=</span><span class="s">"0.4"</span> <span class="na">iyz=</span><span class="s">"0.0"</span> <span class="na">izz=</span><span class="s">"0.2"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/inertial&gt;</span>
  <span class="nt">&lt;/link&gt;</span>

  <span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"right_leg"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;visual&gt;</span>
      <span class="nt">&lt;geometry&gt;</span>
        <span class="nt">&lt;box</span> <span class="na">size=</span><span class="s">"0.6 0.1 0.2"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;/geometry&gt;</span>
      <span class="nt">&lt;origin</span> <span class="na">rpy=</span><span class="s">"0 1.57075 0"</span> <span class="na">xyz=</span><span class="s">"0 0 -0.3"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/visual&gt;</span>
  <span class="nt">&lt;/link&gt;</span>

  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"base_to_right_leg"</span> <span class="na">type=</span><span class="s">"fixed"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"base_link"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"right_leg"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 -0.22 0.25"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>
  
  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"dummy_joint"</span> <span class="na">type=</span><span class="s">"fixed"</span><span class="nt">&gt;</span>
	 <span class="nt">&lt;parent</span> <span class="na">link=</span><span class="s">"dummy"</span><span class="nt">/&gt;</span>
	 <span class="nt">&lt;child</span> <span class="na">link=</span><span class="s">"base_link"</span><span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>

<span class="nt">&lt;/robot&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The above code is a modified example from <code class="language-plaintext highlighter-rouge">urdf_tutorial</code> package, instructions on how to download the tutorials can be found in the ros urdf wiki.</p>

<p>The output of the above code is as follows:</p>

<p><img src="/assets/img/URDF/urdf_1.png" alt="Image1" class="shadow" /></p>

<h4 id="summary-of-the-above-code">Summary of the above code</h4>
<ul>
  <li>The <code class="language-plaintext highlighter-rouge">fixed frame</code> is the transform frame where the center of the grid is located. Here, it’s a frame defined by our one link, <code class="language-plaintext highlighter-rouge">base_link</code>.</li>
  <li>The visual element (the <code class="language-plaintext highlighter-rouge">cylinder</code>) has its <code class="language-plaintext highlighter-rouge">origin</code> at the center of its geometry as a default. Hence, half the cylinder is below the grid.</li>
  <li>The <code class="language-plaintext highlighter-rouge">joint</code> is defined in terms of a <code class="language-plaintext highlighter-rouge">parent</code> and a <code class="language-plaintext highlighter-rouge">child</code>. URDF is ultimately a tree structure with one root link. This means that the leg’s position is dependent on the <code class="language-plaintext highlighter-rouge">base_link</code>’s position.</li>
  <li>Let’s start by examining the joint’s <code class="language-plaintext highlighter-rouge">origin</code>. It is defined in terms of the parent’s reference frame. So we are <code class="language-plaintext highlighter-rouge">-0.22</code> meters in the y direction (to our left, but to the right relative to the axes) and <code class="language-plaintext highlighter-rouge">0.25</code> meters in the z direction (up). This means that the <code class="language-plaintext highlighter-rouge">origin</code> for the child link will be up and to the right, regardless of the child link’s visual <code class="language-plaintext highlighter-rouge">origin</code> tag. Since we didn’t specify a <code class="language-plaintext highlighter-rouge">rpy</code> (roll pitch yaw) attribute, the child frame will be default have the same orientation as the parent frame.</li>
  <li>Now, looking at the leg’s visual <code class="language-plaintext highlighter-rouge">origin</code>, it has both a <code class="language-plaintext highlighter-rouge">xyz</code> and <code class="language-plaintext highlighter-rouge">rpy</code> offset. This defines where the center of the visual element should be, relative to its <code class="language-plaintext highlighter-rouge">origin</code>. Since we want the leg to attach at the top, we offset the <code class="language-plaintext highlighter-rouge">origin</code> down by setting the z offset to be <code class="language-plaintext highlighter-rouge">-0.3</code> meters. And since we want the long part of the leg to be parallel to the z axis, we rotate the visual part PI/2 around the Y axis.</li>
  <li>The collision element is a direct subelement of the link object, at the same level as the visual tag.</li>
  <li>The collision element defines its shape the same way the visual element does, with a geometry tag. The format for the geometry tag is exactly the same here as with the visual.</li>
  <li>You can also specify an origin in the same way as a subelement of the collision tag (as with the visual).</li>
  <li>The <code class="language-plaintext highlighter-rouge">dummy</code> link and joint are added to avoid the following warning given by KDL
    <blockquote>
      <p>The root link base_link has an inertia specified in the URDF, but KDL does not support a root link with an inertia. As a workaround, you can add an extra dummy link to your URDF.</p>
    </blockquote>
  </li>
</ul>

<p>The meshes can be imported in a number of different formats. STL is fairly common, but the engine also supports DAE, which can have its own color data, meaning you don’t have to specify the color/material. Often these are in separate files. These meshes reference the .tif files also in the meshes folder.</p>

<h1 id="xacro--urdf">Xacro + URDF</h1>
<p>The Xacro package can be used to simplify URDF, in paticular it can be used to deal with the following
you can use the xacro package to make your life simpler. It does three things that are very helpful.</p>

<ul>
  <li>Constants</li>
  <li>Simple Math</li>
  <li>Macros</li>
</ul>

<h2 id="using-xacro">Using Xacro</h2>
<h3 id="via-xacro">via .xacro</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="cp">&lt;?xml version="1.0"?&gt;</span>
<span class="nt">&lt;robot</span> <span class="na">xmlns:xacro=</span><span class="s">"http://www.ros.org/wiki/xacro"</span> <span class="na">name=</span><span class="s">"firefighter"</span><span class="nt">&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>At the top of the URDF file, you must specify a namespace in order for the file to parse properly. For example, these are the first two lines of a valid xacro file:</li>
</ul>

<h3 id="via-launch-files">via launch files</h3>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;param</span> <span class="na">name=</span><span class="s">"robot_description"</span>
  <span class="na">command=</span><span class="s">"xacro --inorder '$(find pr2_description)/robots/pr2.urdf.xacro'"</span> <span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>You can also automatically generate the urdf in a launch file.</li>
</ul>

<h2 id="constants">Constants</h2>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:property</span> <span class="na">name=</span><span class="s">"width"</span> <span class="na">value=</span><span class="s">"0.2"</span> <span class="nt">/&gt;</span>
<span class="nt">&lt;xacro:property</span> <span class="na">name=</span><span class="s">"bodylen"</span> <span class="na">value=</span><span class="s">"0.6"</span> <span class="nt">/&gt;</span>

<span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"base_link"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;visual&gt;</span>
        <span class="nt">&lt;geometry&gt;</span>
            <span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">"${width}"</span> <span class="na">length=</span><span class="s">"${bodylen}"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/geometry&gt;</span>
        <span class="nt">&lt;material</span> <span class="na">name=</span><span class="s">"blue"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/visual&gt;</span>
    <span class="nt">&lt;collision&gt;</span>
        <span class="nt">&lt;geometry&gt;</span>
            <span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">"${width}"</span> <span class="na">length=</span><span class="s">"${bodylen}"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/geometry&gt;</span>
    <span class="nt">&lt;/collision&gt;</span>
<span class="nt">&lt;/link&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>
    <p>The two values are specified in the first two lines. They can be defined just about anywhere (assuming valid XML), at any level, before or after they are used. Usually they go at the top.</p>
  </li>
  <li>
    <p>Instead of specifying the actual radius in the geometry element, we use a dollar sign and curly brackets to signify the value.</p>
  </li>
</ul>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:property</span> <span class="na">name=</span><span class="s">”robotname”</span> <span class="na">value=</span><span class="s">”marvin”</span> <span class="nt">/&gt;</span>
<span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">”${robotname}s_leg”</span> <span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>will be read as  <code class="language-plaintext highlighter-rouge">&lt;link name=”marvins_leg” /&gt;</code></p>

<h2 id="maths">Maths</h2>
<p>You can build up arbitrarily complex expressions in the ${} construct using the four basic operations (+,-,*,/), the unary minus, and parenthesis.</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">"${wheeldiam/2}"</span> <span class="na">length=</span><span class="s">"0.1"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"${reflect*(width+.02)} 0 0.25"</span> <span class="nt">/&gt;</span>
<span class="nt">&lt;box</span> <span class="na">size=</span><span class="s">"${cos(pi/6)}$"</span><span class="nt">&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h2 id="macros">Macros</h2>
<h3 id="simple-macro">Simple Macro</h3>
<p>The steup</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:macro</span> <span class="na">name=</span><span class="s">"default_origin"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 0 0"</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/xacro:macro&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The usage:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:default_origin</span> <span class="nt">/&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The output:
<code class="language-plaintext highlighter-rouge">&lt;origin rpy="0 0 0" xyz="0 0 0"/&gt;</code></p>

<ul>
  <li>Every instance of the &lt;xacro:$NAME /&gt; is replaced with the contents of the xacro:macro tag.</li>
  <li>Note that even though its not exactly the same (the two attributes have switched order), the generated XML is equivalent.</li>
  <li>If the xacro with a specified name is not found, it will not be expanded and will NOT generate an error.</li>
</ul>

<h3 id="macro-with-paramters">Macro with Paramters</h3>

<p>The setup:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:macro</span> <span class="na">name=</span><span class="s">"blue_shape"</span> <span class="na">params=</span><span class="s">"name *shape"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"${name}"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;visual&gt;</span>
            <span class="nt">&lt;geometry&gt;</span>
                <span class="nt">&lt;xacro:insert_block</span> <span class="na">name=</span><span class="s">"shape"</span> <span class="nt">/&gt;</span>
            <span class="nt">&lt;/geometry&gt;</span>
            <span class="nt">&lt;material</span> <span class="na">name=</span><span class="s">"blue"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/visual&gt;</span>
        <span class="nt">&lt;collision&gt;</span>
            <span class="nt">&lt;geometry&gt;</span>
                <span class="nt">&lt;xacro:insert_block</span> <span class="na">name=</span><span class="s">"shape"</span> <span class="nt">/&gt;</span>
            <span class="nt">&lt;/geometry&gt;</span>
        <span class="nt">&lt;/collision&gt;</span>
    <span class="nt">&lt;/link&gt;</span>
<span class="nt">&lt;/xacro:macro&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The usage:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:blue_shape</span> <span class="na">name=</span><span class="s">"base_link"</span><span class="nt">&gt;</span>songs
    <span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">".42"</span> <span class="na">length=</span><span class="s">".01"</span> <span class="nt">/&gt;</span>
<span class="nt">&lt;/xacro:blue_shape&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The result:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>    <span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"base_link"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;visual&gt;</span>
            <span class="nt">&lt;geometry&gt;</span>
                <span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">".42"</span> <span class="na">length=</span><span class="s">".01"</span> <span class="nt">/&gt;</span>
            <span class="nt">&lt;/geometry&gt;</span>
            <span class="nt">&lt;material</span> <span class="na">name=</span><span class="s">"blue"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/visual&gt;</span>
        <span class="nt">&lt;collision&gt;</span>
            <span class="nt">&lt;geometry&gt;</span>
                <span class="nt">&lt;cylinder</span> <span class="na">radius=</span><span class="s">".42"</span> <span class="na">length=</span><span class="s">".01"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;/geometry&gt;</span>
        <span class="nt">&lt;/collision&gt;</span>
    <span class="nt">&lt;/link&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>To specify a block parameter, include an asterisk before its parameter name.</li>
  <li>A block can be inserted using the insert_block command</li>
</ul>

<p>The setup:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:macro</span> <span class="na">name=</span><span class="s">"pr2_caster"</span> <span class="na">params=</span><span class="s">"suffix *origin **content **anothercontent"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"caster_${suffix}_joint"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;axis</span> <span class="na">xyz=</span><span class="s">"0 0 1"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;/joint&gt;</span>
  <span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"caster_${suffix}"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;xacro:insert_block</span> <span class="na">name=</span><span class="s">"origin"</span> <span class="nt">/&gt;</span>
    <span class="nt">&lt;xacro:insert_block</span> <span class="na">name=</span><span class="s">"content"</span> <span class="nt">/&gt;</span>
    <span class="nt">&lt;xacro:insert_block</span> <span class="na">name=</span><span class="s">"anothercontent"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;/link&gt;</span>
<span class="nt">&lt;/xacro:macro&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The usage:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;xacro:pr2_caster</span> <span class="na">suffix=</span><span class="s">"front_left"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;pose</span> <span class="na">xyz=</span><span class="s">"0 1 0"</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;container&gt;</span>
    <span class="nt">&lt;color</span> <span class="na">name=</span><span class="s">"yellow"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;mass&gt;</span>0.1<span class="nt">&lt;/mass&gt;</span>
  <span class="nt">&lt;/container&gt;</span>
  <span class="nt">&lt;another&gt;</span>
    <span class="nt">&lt;inertial&gt;</span>
      <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 0 0.5"</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;mass</span> <span class="na">value=</span><span class="s">"1"</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;inertia</span> <span class="na">ixx=</span><span class="s">"100"</span>  <span class="na">ixy=</span><span class="s">"0"</span>  <span class="na">ixz=</span><span class="s">"0"</span> <span class="na">iyy=</span><span class="s">"100"</span> <span class="na">iyz=</span><span class="s">"0"</span> <span class="na">izz=</span><span class="s">"100"</span> <span class="nt">/&gt;</span>
    <span class="nt">&lt;/inertial&gt;</span>
  <span class="nt">&lt;/another&gt;</span>
<span class="nt">&lt;/xacro:pr2_caster&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The result:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;joint</span> <span class="na">name=</span><span class="s">"caster_front_left_joint"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;axis</span> <span class="na">xyz=</span><span class="s">"0 0 1"</span> <span class="nt">/&gt;</span>
<span class="nt">&lt;/joint&gt;</span>
<span class="nt">&lt;link</span> <span class="na">name=</span><span class="s">"caster_front_left"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;pose</span> <span class="na">xyz=</span><span class="s">"0 1 0"</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;color</span> <span class="na">name=</span><span class="s">"yellow"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;mass&gt;</span>0.1<span class="nt">&lt;/mass&gt;</span>
  <span class="nt">&lt;inertial&gt;</span>
    <span class="nt">&lt;origin</span> <span class="na">xyz=</span><span class="s">"0 0 0.5"</span> <span class="na">rpy=</span><span class="s">"0 0 0"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;mass</span> <span class="na">value=</span><span class="s">"1"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;inertia</span> <span class="na">ixx=</span><span class="s">"100"</span>  <span class="na">ixy=</span><span class="s">"0"</span>  <span class="na">ixz=</span><span class="s">"0"</span> <span class="na">iyy=</span><span class="s">"100"</span> <span class="na">iyz=</span><span class="s">"0"</span> <span class="na">izz=</span><span class="s">"100"</span> <span class="nt">/&gt;</span>
  <span class="nt">&lt;/inertial&gt;</span>
<span class="nt">&lt;/link&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="ros-control">ROS Control</h1>
<p>Coming soon…</p>]]></content><author><name>Jai Krishna</name></author><category term="Resources" /><category term="Robotics Theory" /><category term="ws2812b" /><category term="esp32" /><category term="mic" /><category term="bluetooth" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/Ros_logo.png" /><media:content medium="image" url="https://textzip.github.io/assets/img/Ros_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lite Bar</title><link href="https://textzip.github.io/posts/LiteBar/" rel="alternate" type="text/html" title="Lite Bar" /><published>2021-12-15T19:13:20+05:30</published><updated>2022-01-13T19:05:02+05:30</updated><id>https://textzip.github.io/posts/LiteBar</id><content type="html" xml:base="https://textzip.github.io/posts/LiteBar/"><![CDATA[<p><img src="/assets/img/LiteBar/closeup.jpg" alt="Image1" class="shadow" /></p>

<h1 id="introduction">Introduction</h1>
<p>A multipurpose indivudially addressable LED Strip based tubelight, that can be used to visualize music, time, smartphone notifications.</p>

<p><a href="https://github.com/TextZip/LiteBar"><img src="https://gh-card.dev/repos/TextZip/LiteBar.svg" alt="TextZip/LiteBar - GitHub" /></a></p>

<h1 id="hardware">Hardware</h1>
<ul>
  <li>ESP32</li>
  <li>1 Meter WS2812B Strip (144 LEDS per meter density)</li>
  <li>Microphone Module</li>
  <li>SMPS</li>
  <li>RTC Module</li>
  <li>LED Tubelight + holder</li>
  <li>Basic power tools</li>
</ul>

<h1 id="overview">Overview</h1>

<p><img src="/assets/img/LiteBar/flowchart.png" alt="Image1" class="shadow" /></p>

<!-- Jekyll Ideal Image Slider Include -->
<!-- https://github.com/jekylltools/jekyll-ideal-image-slider-include -->
<!-- v1.8 -->
<div id="litebar">
  <img data-src="" data-src-2x="" src="/assets/img/LiteBar/bench_1.jpg" title="" alt="Tubelight and LED Strip seperated" />
  <img data-src="" data-src-2x="" src="/assets/img/LiteBar/bench_2.jpg" title="" alt="Tubelight and LED Strip seperated" />
  <img data-src="" data-src-2x="" src="/assets/img/LiteBar/bench_smps.jpg" title="" alt="Modified Tubelight" />
  <img data-src="" data-src-2x="" src="/assets/img/LiteBar/test.jpg" title="" alt="Testing" />
  <img data-src="" data-src-2x="" src="/assets/img/LiteBar/display.jpg" title="" alt="Testing" />
  </div>

<p>Build details and an overview of the programming will be updated soon.</p>

<h1 id="results">Results</h1>

<iframe width="640" height="385" src="https://youtube.com/embed/vQ962GT1HcI" frameborder="0" allowfullscreen=""></iframe>]]></content><author><name>Jai Krishna</name></author><category term="Projects" /><category term="DIY" /><category term="ws2812b" /><category term="esp32" /><category term="mic" /><category term="bluetooth" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://textzip.github.io/assets/img/LiteBar/thumbnail.jpg" /><media:content medium="image" url="https://textzip.github.io/assets/img/LiteBar/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>