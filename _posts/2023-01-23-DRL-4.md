---
title: Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods
date: 2023-01-23 13:13:20 +/-0530
categories: [Resources, Deep Reinforcement Learning]
tags: [mdp,optimal value,bellman]     # TAG names should always be lowercase
math: true
image: /assets/img/drl_logo.jpg
---
This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check [Part 0 - Getting Started](https://textzip.github.io/posts/DRL-0/).

### Structure of the blog 
* [Part 0 - Getting Started](https://textzip.github.io/posts/DRL-0/)
* [Part 1 - Multi-Arm Bandits](https://textzip.github.io/posts/DRL-1/)
* [Part 2 - Finite MDP](https://textzip.github.io/posts/DRL-2/) \| **Prerequisites**: *python, gymnasium, numpy*
* [Part 3 - Dynamic Programming](https://textzip.github.io/posts/DRL-3/) \| **Prerequisites**: *python, gymnasium, numpy*
* [Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods](https://textzip.github.io/posts/DRL-4/) \| **Prerequisites**: *Python, gymnasium, numpy*
* Part 5 - Deep SARSA and Q-Learning \| **Prerequisites**: *python, gymnasium, numpy, torch*
* Part 6 - REINFORCE, AC, DDPG, TD3, SAC
* Part 7 - A2C, PPO, TRPO, GAE, A3C
* TBA (HER, PER, Distillation)

# Monte Carlo Methods
MC methods improvise over DP methods as they can be used in cases where we do not have a model of the environment. They do this by learning from episodes of experience. Therefore one caviat of MC methods is that they do not work on continous MDPs and learn only from complete episodes (Episodes must terminate).

## Monte Carlo Prediction
We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.
### On-Policy MC Prediction
We have a small variation in the fact that we can consider each state only the first time it has been visited while estimating the mean return or we can account for multiple visits to the same state(if any) and the follwoing pseudocodes illustrate both the variations 
![image1](/assets/img/DRL4/first-visit-mc-pred.png){: .shadow}

The every-visit version can be implemented by removing the "Unless $S_t$ appears in $S_0$, $S_1$, ... $S_{t-1}$" line.

#### Incremental Updates 
A more computationally efficient method would be to calculate the mean incrementally as follows:

- Update $V(s)$ incrementally after episode $S_1,A_1,R_2,....,S_T$
- For each state $S_t$ with return $G_t$

$$ N(S_t) \leftarrow  N(S_t) + 1 $$


$$ V(S_t) \leftarrow  V(S_t) + \dfrac{1}{N(S_t)}(G_t-V(S_t)) $$

For non-stationary problems, it can be useful to track a running mean (forgets old epiosdes and gives more weight to recent experiences).

$$ V(S_t) \leftarrow  V(S_t) + \alpha(G_t-V(S_t)) $$


### Off-Policy MC Prediction
Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Off-Policy Monte Carlo Prediction can be implemented via two variations of importance sampling which are discussed below.
#### Ordinary Importance Sampling
For evaluating a terget policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s,a)$ while following a behaviour policy $\mu(a|s)$.

Given, 
$$\{S_1,A_1,R_2,...,S_T\} \sim \mu$$

We can weight returns $G_t$ according to similarity between the two policies. By multiplying the importance sampling corrections along the whole episode we get:

$$ G_t^{\pi/\mu} = \dfrac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})...\pi(A_{T}|S_{T})}{\mu(A_{t}|S_{t})\mu(A_{t+1}|S_{t+1})...\mu(A_{T}|S_{T})}G_t$$

We can then update the state value towards the corrected return like this 

$$ V(S_t) \leftarrow  V(S_t) + \alpha(G_t^{\pi/\mu}-V(S_t)) $$

Note that we cannot use this if $\mu$ is zero when $\pi$ is non-zero, also that importance sampling can increase variance. 

#### Weighted Importance Sampling 
![image1](/assets/img/DRL4/off-policy-mc-weight-sample-pred.png){: .shadow}
The derivation of the Weighted Importance Sampling equations has been left-out for the time being.

Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges
asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and
Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.
## Monte Carlo Control
While it might seem straight forward to implement MC Methods in GPI by plugging MC Prediction for policy evaluation and using greedy policy improvement to complete the cycle, there is one key problem that needs to be addressed. 

Greedy policy improvement over $V(s)$ requires knowledge about the MDP
$$\pi'(s) = \mathtt{argmax}_{a\in A}  r(a|s) + p(s'|s,a)V(s') $$

To remain model free we can instead switch to action value functions which will not require prior details about the MDP

$$\pi'(s) = \mathtt{argmax}_{a\in A} Q(s,a)$$

While this solves the issue of knowing the model MDP, we now have a deterministic policy and we will never be able to collect experiences of alternative actions and therefore might miss out on exploration altogether. 

This can be solved in the following ways:

- **Exploring Starts:** Every state-action pair has a non-zero probability of being selected as the starting pair, this ensures sufficient exploration but in reality, this might not always be possible. 

- **$\epsilon-$ soft policies:** A small probability to explore every time an action is to be choosen.

- **Off-Policy:** Use a different policy to collect experience than the one target policy being improved.

### On-Policy MC Control
#### Exploring Starts
The pseudocode for exploring starts can be found below:
![image1](/assets/img/DRL4/mc-es-control.png){: .shadow}

#### On-Policy First Visit MC Control
The pseudocode for On-Policy First Visit MC Control can be found below:
![image1](/assets/img/DRL4/on-policy-fv-mc-control.png){: .shadow}

The python implementation for the following can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo [drl-algorithms](https://github.com/TextZip/drl-algorithms). 

```python
import gymnasium as gym
import numpy as np
env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
               is_slippery=False, render_mode="rgb_array")

action_values = np.zeros((env.observation_space.n,env.action_space.n))

def policy(state,epsilon=0.2):
    if np.random.random()<epsilon:
        return np.random.choice(env.action_space.n)
    else:
        action_value = action_values[state]
        return np.random.choice(np.flatnonzero(action_value == action_value.max()))

def on_policy_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2):
    sa_returns = {}

    for episode in range(1, episodes+1):
        state,info = env.reset()
        done = False
        transitions = []

        while not done:
            action = policy(state=state,epsilon=epsilon)
            next_state,reward,terminated,truncated,info = env.step(action=action)
            done = terminated or truncated
            transitions.append([state,action,reward])
            state = next_state

        G = 0

        for state_t,action_t,reward_t in reversed(transitions):
            G = reward_t + gamma*G

            if not (state_t,action_t) in sa_returns:
                sa_returns[(state_t,action_t)] = []
                sa_returns[(state_t,action_t)].append(G)
                action_values[state_t,action_t] = np.mean(sa_returns[(state_t,action_t)])
    env.close()


on_policy_monte_carlo(policy=policy,action_values=action_values,episodes=10000)

env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
               is_slippery=False, render_mode="human")
observation,info = env.reset()
terminated = False
while not terminated:
    action = policy(observation,epsilon=0)
    observation, reward, terminated, truncated, info = env.step(action=action)
env.close()
```
#### On-Policy Every Visit MC Control
On-Policy Every Visit MC Control can be implemented by making a small change to the inner loop of the above code for the first visit version as follows:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo [drl-algorithms](https://github.com/TextZip/drl-algorithms). 

```python
def on_policy_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2):
    sa_returns = {}

    for episode in range(1, episodes+1):
        state,info = env.reset()
        done = False
        transitions = []

        while not done:
            action = policy(state=state,epsilon=epsilon)
            next_state,reward,terminated,truncated,info = env.step(action=action)
            done = terminated or truncated
            transitions.append([state,action,reward])
            state = next_state

        G = 0

        for state_t,action_t,reward_t in reversed(transitions):
            G = reward_t + gamma*G

            if not (state_t,action_t) in sa_returns:
                sa_returns[(state_t,action_t)] = []
            sa_returns[(state_t,action_t)].append(G)
            action_values[state_t,action_t] = np.mean(sa_returns[(state_t,action_t)])
    env.close()

```

#### On-Policy Every Visit Constant Alpha MC Control
The constant alpha version is based on the idea of using a running mean instead of using a normal return to deal with non-stationary problems. 

The major change being the following equation: 
$$ Q(S_t|A_t) \leftarrow  Q(S_t|A_t) + \alpha(G_t-Q(S_t|A_t)) $$

The python implementation can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo [drl-algorithms](https://github.com/TextZip/drl-algorithms). 

```python
import gymnasium as gym
import numpy as np
env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
               is_slippery=False, render_mode="rgb_array")

action_values = np.zeros((env.observation_space.n,env.action_space.n))

def policy(state,epsilon=0.2):
    if np.random.random()<epsilon:
        return np.random.choice(env.action_space.n)
    else:
        action_value = action_values[state]
        return np.random.choice(np.flatnonzero(action_value == action_value.max()))

def constant_alpha_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2,alpha=0.2):

    for episode in range(1, episodes+1):
        state,info = env.reset()
        done = False
        transitions = []

        while not done:
            action = policy(state=state,epsilon=epsilon)
            next_state,reward,terminated,truncated,info = env.step(action=action)
            done = terminated or truncated
            transitions.append([state,action,reward])
            state = next_state

        G = 0

        for state_t,action_t,reward_t in reversed(transitions):
            G = reward_t + gamma*G

            old_value = action_values[state_t,action_t]
            action_values[state_t,action_t] += alpha*(G-old_value)

    env.close()


constant_alpha_monte_carlo(policy=policy,action_values=action_values,episodes=20000)

env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
               is_slippery=False, render_mode="human")
observation,info = env.reset()
terminated = False
while not terminated:
    action = policy(observation,epsilon=0)
    observation, reward, terminated, truncated, info = env.step(action=action)
env.close()
```

### Off-Policy MC Control
In Off-Policy methods, the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can
continue to sample all possible actions. 

Off-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).

![image1](/assets/img/DRL4/off-policy-mc-control.png){: .shadow}

The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo [drl-algorithms](https://github.com/TextZip/drl-algorithms). 
```python
import gymnasium as gym
import numpy as np
env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
                is_slippery=False, render_mode="rgb_array")

action_values = np.zeros((env.observation_space.n,env.action_space.n))

def target_policy(state):
    action_value = action_values[state]
    return np.random.choice(np.flatnonzero(action_value == action_value.max()))

def exploratory_policy(state,epsilon=0.2):
    if np.random.random()<epsilon:
        return np.random.choice(env.action_space.n)
    else:
        action_value = action_values[state]
        return np.random.choice(np.flatnonzero(action_value == action_value.max()))


def off_policy_monte_carlo(action_values,target_policy,exploratory_ploicy,episodes,gamma=0.99,epsilon=0.2):
    counter_sa_values = np.zeros((env.observation_space.n,env.action_space.n))

    for episode in range(1,episodes+1):
        state,_ = env.reset()
        done = False
        transitions = [] 

        while not done:
            action = exploratory_ploicy(state=state,epsilon=epsilon)
            next_state,reward,terminated,truncated,_ = env.step(action=action)
            done = terminated or truncated
            transitions.append([state,action,reward])
            state = next_state
        
        G = 0 
        W = 1     

        for state_t,action_t,reward_t in reversed(transitions):
            G = reward_t + gamma*G
            counter_sa_values[state_t,action_t] += W
            old_value = action_values[state_t,action_t]
            action_values[state_t,action_t] += (W/counter_sa_values[state_t,action_t])* (G - old_value)

            if action_t != target_policy(state_t):
                break

            W = W*(1/(1-epsilon + (epsilon/4)))
    env.close()

off_policy_monte_carlo(action_values=action_values,target_policy=target_policy,exploratory_ploicy=exploratory_policy,episodes=5000)

env = gym.make('FrozenLake-v1', desc=None, map_name="8x8",
               is_slippery=False, render_mode="human")
observation,_ = env.reset()
terminated = False
while not terminated:
    action = target_policy(observation)
    observation, reward, terminated, truncated, info = env.step(action=action)
env.close()
```

# Temporal Difference Methods
## Temporal Difference Prediction 
### On-Policy TD Prediction
### Off-Policy TD Prediction
## Temporal Difference Control
### On-Policy TD Control (SARSA)
### Off-Policy TD Control (Q-Learning)


# Bootstrapping 


