---
title: Deep Reinforcement Learning - Part 3 - Dynamic Programming
date: 2022-11-16 13:13:20 +/-0530
categories: [Resources, Deep Reinforcement Learning]
tags: [mdp,optimal value,bellman]     # TAG names should always be lowercase
math: true
image: /assets/img/drl_logo.png
---
This post is part of the deep reinforcement learning series that I am writing and is based on the resources listed below:
- *Richard S. Sutton and Andrew G. Barto. 2018. "Reinforcement Learning: An Introduction." A Bradford Book, Cambridge, MA, USA.*
- *Example codes and Algorithm Implementations by Myself*
- *David Silver - Reinforcement Lectures*

# Dynamic Programming
The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the Approx. methods presented in later parts. In
fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.

## Policy Evaluation 
First we consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.

Recall that, 

$$v_\pi(s) = \sum_{a}\pi(a\mid s)\sum_{s',r} p(s',r\mid a,s)[r+\gamma v_\pi(s')] $$

## Policy Improvement 

## Policy Iteration

## Value Iteration

## Async. Dynamic Programming

## Generalized Policy Iteration

## Efficiency of Dynamic Programming